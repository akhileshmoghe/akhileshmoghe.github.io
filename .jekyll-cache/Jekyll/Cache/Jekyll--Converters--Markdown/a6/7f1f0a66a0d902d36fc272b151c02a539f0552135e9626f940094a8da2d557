I"<p>This write-up will only focus on AWS Sagemaker ML service with respect to ML model deployment to Edge devices and Cloud.</p>

<h2 id="aws-sagemaker-neo">AWS Sagemaker Neo</h2>
<ul>
  <li>Amazon SageMaker Neo enables developers to optimize machine learning (ML) models for inference on SageMaker in the cloud and supported devices at the edge for the specific underlying hardware.</li>
  <li>Optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy.</li>
  <li>Amazon SageMaker <strong><em><u>Neo runtime</u></em></strong> is supported on <em><u>Android</u></em>, <em><u>iOS</u></em>, <em><u>Linux</u></em>, and <em><u>Windows</u></em> operating systems.</li>
  <li>Sagemaker neo can optimize the ML model to run on <em><u>target hardware platform</u></em> of Edge devices based on processors from <em><u>Ambarella</u></em>, <em><u>Apple</u></em>, <em><u>ARM</u></em>, <em><u>Intel</u></em>, <em><u>MediaTek</u></em>, <em><u>Nvidia</u></em>, <em><u>NXP</u></em>, <em><u>Qualcomm</u></em>, <em><u>RockChip</u></em>, <em><u>Texas Instruments</u></em>, or <em><u>Xilinx</u></em>.</li>
  <li>Compiles it into an <em><u>executable</u></em>.</li>
  <li>For inference in the cloud, SageMaker Neo speeds up inference and saves cost by creating <em><u>an inference optimized container</u></em> that include <em><u>MXNet</u></em>, <em><u>PyTorch</u></em>, and <em><u>TensorFlow</u></em> integrated with Neo runtime for SageMaker hosting.</li>
  <li>Amazon SageMaker Neo supports optimization for a model from the framework-specific format of <em><u>DarkNet</u></em>, <em><u>Keras</u></em>, <em><u>MXNet</u></em>, <em><u>PyTorch</u></em>, <em><u>TensorFlow</u></em>, <em><u>TensorFlow-Lite</u></em>, <em><u>ONNX</u></em>, or <em><u>XGBoost</u></em>.</li>
  <li>Amazon SageMaker <em><u>Neo runtime</u></em> occupies 1MB of storage and 2MB of memory, which is many times smaller than the storage and memory footprint of a framework, while providing a simple common <em><u>API to run a compiled model</u></em> originating in any framework.</li>
  <li>Amazon SageMaker Neo takes advantage of partner-provided <u>accelerator libraries</u> to deliver the best available performance for a deep learning model on heterogeneous hardware platforms with a <u>hardware accelerator</u> as well as a CPU. Acceleration libraries such as <em><u>Ambarella CV Tools</u></em>, <em><u>Nvidia Tensor RT</u></em>, and <em><u>Texas Instruments TIDL</u></em> each support a specific set of functions and operators. SageMaker Neo automatically partitions your model so that the part with operators supported by the accelerator can run on the accelerator while the rest of the model runs on the CPU.</li>
  <li>Amazon SageMaker Neo now compiles models for Amazon SageMaker <strong><em><u>INF1 instance</u></em></strong> targets. SageMaker hosting provides a managed service for inference on the INF1 instances, which are based on the <strong><em><u>AWS Inferentia chip</u></em></strong>.</li>
</ul>

<h3 id="references">References</h3>
<ul>
  <li><a href="https://aws.amazon.com/sagemaker/neo/">Amazon SageMaker Neo</a></li>
</ul>

<h2 id="aws-sagemaker-edge-manager">AWS Sagemaker Edge Manager</h2>
<ul>
  <li>AWS Sagemaker Edge Manager consists of a Service running in AWS cloud and an Agent running on Edge devices.</li>
  <li>Sagemaker Edge Manager deploys a ML model <em><u>optimized</u></em> with <em><u>SageMaker Neo</u></em> automatically so you donâ€™t need to have Neo runtime installed on your devices in order to take advantage of the model optimizations.</li>
</ul>

<h3 id="agent">Agent</h3>
<ul>
  <li>Use the agent to <em><u>make predictions</u></em> with models loaded onto your edge devices.</li>
  <li>The agent also <em><u>collects model metrics</u> and *<u>captures data</u></em> at specific intervals.</li>
  <li>Sample data is stored in your <strong><em><u>Amazon S3</u></em></strong> bucket.</li>
  <li>2 methods of installing and deploying the Edge Manager agent onto your edge devices:
    <ul>
      <li>Download the <em><u>agent as a binary</u></em> from the Amazon S3 release bucket.</li>
      <li>Use the <strong><em><u>AWS IoT Greengrass V2</u></em></strong> console or the <strong><em><u>AWS CLI</u></em></strong> to deploy aws.greengrass.SageMakerEdgeManager.</li>
    </ul>
  </li>
</ul>

<h3 id="monitoring-deployments-across-fleets">Monitoring deployments across fleets</h3>
<ul>
  <li>SageMaker Edge Manager also <em><u>collects prediction data and sends</u></em> a sample of the data to the cloud for monitoring, labeling, and retraining.</li>
  <li>All data can be viewed in the <strong><em><u>SageMaker Edge Manager dashboard</u></em></strong> which reports on the operation of deployed models.</li>
  <li>The dashboard is useful to understand the performance of models running on each device across your fleet, understand overall fleet health and identify problematic models and particular devices.</li>
  <li>If quality declines are detected, you can quickly spot them in the dashboard and also configure alerts through <strong><em><u>Amazon CloudWatch</u></em></strong>.</li>
</ul>

<h3 id="signed-and-verifiable-ml-deployments">Signed and Verifiable ML deployments</h3>
<ul>
  <li>SageMaker Edge Manager also <em><u>cryptographically signs your models</u></em> so you can verify that it was not tampered with as it moves from the cloud to edge devices.</li>
</ul>

<h3 id="integration-with-device-applications">Integration with device applications</h3>
<ul>
  <li>SageMaker Edge Manager <em><u>supports</u></em> <strong><em><u>gRPC</u></em></strong>, an open source <em><u>remote procedure call</u></em>, which allows you to integrate SageMaker Edge Manager with your existing edge applications through APIs in common programming languages, such as Android Java, C# / .NET, Dart, Go, Java, Kotlin/JVM, Node.js, Objective-C, PHP, Python, Ruby, and Web.</li>
  <li>Manages models separately from the rest of the application, so that updates to the model and the application are independent.
    <h3 id="multiple-ml-models-serve-on-edge-devices">Multiple ML models serve on edge devices</h3>
  </li>
  <li>ML applications usually require hosting and running multiple models concurrently on a device.</li>
  <li>SageMaker Edge Manager will soon allow you to write <em>simple application logic</em> to send one or more queries (i.e. load/unload models, run inference) <em><u>independently to multiple models</u></em> and <em><u>rebalance hardware resource utilization</u></em> when you add or update a model.</li>
</ul>

<h3 id="model-registry-and-lifecycle">Model Registry and Lifecycle</h3>
<ul>
  <li>SageMaker Edge Manager will soon be able to automate the build-train-deploy workflow from cloud to edge devices in Amazon SageMaker Edge Manager, and trace the lifecycle of each model.</li>
</ul>

<h3 id="reference">Reference</h3>
<ul>
  <li><a href="https://www.youtube.com/watch?v=zS0Q3bdsLiU&amp;t=3s&amp;ab_channel=AmazonWebServices">Sagemaker Edge Manager YouTube</a></li>
  <li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html">Developer Guide: SageMaker Edge Manager</a></li>
  <li><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/edge-device-fleet-about.html">Developer Guide: Edge Manager Agent</a></li>
  <li><a href="https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_edge_manager/sagemaker_edge_example/sagemaker_edge_example.html?highlight=edge">SageMaker Edge Manager Example</a></li>
</ul>

<h2 id="aws-outpost-and-aws-sagemaker-edge-manager">AWS Outpost and AWS Sagemaker Edge Manager</h2>
<ul>
  <li><a href="https://aws.amazon.com/blogs/machine-learning/machine-learning-at-the-edge-with-aws-outposts-and-amazon-sagemaker/">Machine Learning at the Edge with AWS Outposts and Amazon SageMaker</a>
  <img src="/assets/images/aws/aws-outpost-sagemaker-edge-manager.png" alt="aws-outpost-sagemaker-edge-manager" /></li>
</ul>

:ET