<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-12-11T23:51:15+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Every THING is Internet of Thing</title><subtitle>A Journey in the direction of ---&gt; IoT ---&gt; Edge ---&gt; Cloud ---&gt; DevOps ---&gt; ML ---&gt; AI
</subtitle><author><name>Akhilesh Moghe</name><email>akhileshmoghe@live.com</email></author><entry><title type="html">Design Patterns: Most Important GRASP Patterns of designing an application or module</title><link href="http://localhost:4000/c++/design%20patterns/2021/12/07/C++_Design_Patterns_GRASP_Patterns.html" rel="alternate" type="text/html" title="Design Patterns: Most Important GRASP Patterns of designing an application or module" /><published>2021-12-07T05:33:22+05:30</published><updated>2021-12-07T05:33:22+05:30</updated><id>http://localhost:4000/c++/design%20patterns/2021/12/07/C++_Design_Patterns_GRASP_Patterns</id><content type="html" xml:base="http://localhost:4000/c++/design%20patterns/2021/12/07/C++_Design_Patterns_GRASP_Patterns.html">&lt;p&gt;This write-up will explain different &lt;strong&gt;&lt;em&gt;&lt;u&gt;GRASP Patterns&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, explained in book: &lt;a href=&quot;https://www.amazon.com/gp/product/0131489062/ref=as_li_qf_asin_il_tl?ie=UTF8&amp;amp;tag=fluentcpp-20&amp;amp;creative=9325&amp;amp;linkCode=as2&amp;amp;creativeASIN=0131489062&amp;amp;linkId=43cfc4d0a6ea922ef663317e4f91db85&quot;&gt;Applying UML and Patterns&lt;/a&gt;, which are more important in day to day programming life than the whole lot of GoF design patterns.
GRASP is a set of 9 fundamental principles in OOPs design and responsibility assignment.
These patterns solve some software problem common to many software development projects.&lt;/p&gt;

&lt;h2 id=&quot;information-expert-information-hiding&quot;&gt;Information Expert (Information Hiding)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Problem&lt;/u&gt;&lt;/em&gt; it solves: How will you assign a responsibility to a module/class?&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Solution&lt;/u&gt;&lt;/em&gt;: Assign responsibility to the class that has the information needed to fulfill it.&lt;/li&gt;
  &lt;li&gt;Used to determine where to delegate responsibilities such as methods, computed fields, and so on.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;If you have an operation to do, and this operations needs inputs, then you should consider putting the responsibility of carrying out this operation in the class that contains the inputs for it&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Helps keeping the data local, i.e. &lt;strong&gt;&lt;em&gt;Data Encapsulation&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Reduces relationships between the classes. (Class is self-sufficient in terms of data to carryout it’s tasks.)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;creator&quot;&gt;Creator&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The creation of objects is one of the most common activities in an object-oriented system.&lt;/li&gt;
  &lt;li&gt;Which class is responsible for creating objects is a fundamental property of the relationship between objects of particular classes.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;reference&quot;&gt;Reference&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cplusplus.com/reference/forward_list/forward_list/&quot;&gt;std::forward_list&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name><email>akhileshmoghe@live.com</email></author><category term="C++" /><category term="Design Patterns" /><category term="C++" /><category term="Design Patterns" /><summary type="html">This write-up will explain different GRASP Patterns, explained in book: Applying UML and Patterns, which are more important in day to day programming life than the whole lot of GoF design patterns. GRASP is a set of 9 fundamental principles in OOPs design and responsibility assignment. These patterns solve some software problem common to many software development projects. Information Expert (Information Hiding) Problem it solves: How will you assign a responsibility to a module/class? Solution: Assign responsibility to the class that has the information needed to fulfill it. Used to determine where to delegate responsibilities such as methods, computed fields, and so on. If you have an operation to do, and this operations needs inputs, then you should consider putting the responsibility of carrying out this operation in the class that contains the inputs for it. Helps keeping the data local, i.e. Data Encapsulation. Reduces relationships between the classes. (Class is self-sufficient in terms of data to carryout it’s tasks.) Creator The creation of objects is one of the most common activities in an object-oriented system. Which class is responsible for creating objects is a fundamental property of the relationship between objects of particular classes. Reference std::forward_list</summary></entry><entry><title type="html">AWS IoT Twinmaker:</title><link href="http://localhost:4000/posts/twinmaker" rel="alternate" type="text/html" title="AWS IoT Twinmaker:" /><published>2021-12-07T00:00:00+05:30</published><updated>2021-12-07T00:00:00+05:30</updated><id>http://localhost:4000/posts/AWS-IoT-Twinmaker</id><content type="html" xml:base="http://localhost:4000/posts/twinmaker">&lt;p&gt;This write-up will focus on newly launched AWS IoT Twinmaker service.&lt;/p&gt;

&lt;h2 id=&quot;digital-twins&quot;&gt;Digital Twins&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Digital twins are &lt;em&gt;&lt;u&gt;virtual representations of physical systems&lt;/u&gt;&lt;/em&gt; such as buildings, factories, production lines, and equipment that are regularly updated with real-world data to mimic the &lt;strong&gt;structure&lt;/strong&gt;, &lt;strong&gt;state&lt;/strong&gt;, and &lt;strong&gt;behavior&lt;/strong&gt; of the systems they represent.&lt;/li&gt;
  &lt;li&gt;To create and use digital twins of real-world systems to monitor and optimize operations.&lt;/li&gt;
  &lt;li&gt;We can create digital twins of &lt;strong&gt;equipment&lt;/strong&gt;, &lt;strong&gt;processes&lt;/strong&gt;, and &lt;strong&gt;facilities&lt;/strong&gt; by connecting data from different data sources like &lt;em&gt;&lt;u&gt;equipment sensors&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;video feeds&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;business applications&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;digital-twin-graph&quot;&gt;Digital Twin Graph&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AWS IoT TwinMaker forms a digital twin graph that combines and understands the &lt;em&gt;&lt;u&gt;relationships between virtual representations of your physical systems and connected data sources&lt;/u&gt;&lt;/em&gt;, so you can accurately model your real-world environment.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Import&lt;/u&gt;&lt;/em&gt; existing &lt;strong&gt;3D models&lt;/strong&gt; (such as &lt;em&gt;&lt;u&gt;CAD files&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;point cloud scans&lt;/u&gt;&lt;/em&gt;) to compose and arrange &lt;strong&gt;3D scenes&lt;/strong&gt; of a physical space and its contents (e.g. a factory and its equipment) using simple 3D tools.&lt;/li&gt;
  &lt;li&gt;We can add data to these models/scenes for visualization as:
    &lt;ul&gt;
      &lt;li&gt;Interactive video.&lt;/li&gt;
      &lt;li&gt;sensor data overlays from the connected data sources.&lt;/li&gt;
      &lt;li&gt;insights from connected machine learning (ML) and simulation services.&lt;/li&gt;
      &lt;li&gt;equipment maintenance records and manuals.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Using the digital twin graph, customers can now issue geospatial queries such as finding all cameras that are pointing to an equipment to help with root cause analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;amazon-managed-grafana-plugin&quot;&gt;Amazon Managed Grafana plugin&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Can be used to create a web-based application for end users.&lt;/li&gt;
  &lt;li&gt;Use Grafana applications to observe and interact with the digital twin to help them optimize factory operations, increase production output, and improve equipment performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;model-builder&quot;&gt;Model Builder&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Allows you to create &lt;em&gt;&lt;u&gt;workspaces&lt;/u&gt;&lt;/em&gt; that will hold the resources, such as entity models and visual assets needed to create a digital twin.&lt;/li&gt;
  &lt;li&gt;In this workspace, create entities that represent digital replicas of your equipment.&lt;/li&gt;
  &lt;li&gt;Specify custom relationships between these entities to create a digital twin graph of your real-world system.&lt;/li&gt;
  &lt;li&gt;Using the digital twin graph, customers can now issue geospatial queries such as finding all cameras that are pointing to an equipment to help with root cause analysis.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;data-connector&quot;&gt;Data Connector&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Associate entities with connectors (called as &lt;em&gt;components&lt;/em&gt; in AWS IoT TwinMaker) to data stores such as AWS IoT SiteWise, to provide context to the data present in various data stores.&lt;/li&gt;
  &lt;li&gt;Built-in &lt;strong&gt;&lt;em&gt;&lt;u&gt;Data Connectors&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; for the following AWS services:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;AWS IoT SiteWise&lt;/strong&gt; for equipment and &lt;em&gt;&lt;u&gt;time-series sensor data&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon Kinesis Video Streams&lt;/strong&gt; for &lt;em&gt;&lt;u&gt;video data&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon Simple Storage Service (S3)&lt;/strong&gt; for &lt;em&gt;&lt;u&gt;storage of visual resources&lt;/u&gt;&lt;/em&gt; (for example, CAD files) and data from business applications.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Provides a framework for you to create &lt;em&gt;&lt;u&gt;your own Data Connectors&lt;/u&gt;&lt;/em&gt; to use with other data sources (such as &lt;a href=&quot;https://www.snowflake.com/&quot;&gt;Snowflake&lt;/a&gt; and &lt;a href=&quot;https://siemens.mindsphere.io/en&quot;&gt;Siemens MindSphere&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;scene-composer&quot;&gt;Scene Composer&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A tool to create visualizations in 3D.&lt;/li&gt;
  &lt;li&gt;You can bring previously built 3D/CAD models into your resource library in Amazon S3.&lt;/li&gt;
  &lt;li&gt;these visual assets can be brought into a scene, and position the 3D assets to match your real-world systems.&lt;/li&gt;
  &lt;li&gt;visual annotations such as tags on top of the base scene.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;references&quot;&gt;References&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/iot/introducing-aws-iot-twinmaker/&quot;&gt;Introducing AWS IoT TwinMaker&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;further-reference&quot;&gt;Further Reference&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/aws-iot-twinmaker-samples&quot;&gt;aws-samples/aws-iot-twinmaker-samples&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="AWS IoT" /><category term="Digital Twin" /><category term="AWS IoT" /><category term="Digital Twin" /><summary type="html">This write-up will focus on newly launched AWS IoT Twinmaker service. Digital Twins Digital twins are virtual representations of physical systems such as buildings, factories, production lines, and equipment that are regularly updated with real-world data to mimic the structure, state, and behavior of the systems they represent. To create and use digital twins of real-world systems to monitor and optimize operations. We can create digital twins of equipment, processes, and facilities by connecting data from different data sources like equipment sensors, video feeds, and business applications. Digital Twin Graph AWS IoT TwinMaker forms a digital twin graph that combines and understands the relationships between virtual representations of your physical systems and connected data sources, so you can accurately model your real-world environment. Import existing 3D models (such as CAD files, and point cloud scans) to compose and arrange 3D scenes of a physical space and its contents (e.g. a factory and its equipment) using simple 3D tools. We can add data to these models/scenes for visualization as: Interactive video. sensor data overlays from the connected data sources. insights from connected machine learning (ML) and simulation services. equipment maintenance records and manuals. Using the digital twin graph, customers can now issue geospatial queries such as finding all cameras that are pointing to an equipment to help with root cause analysis. Amazon Managed Grafana plugin Can be used to create a web-based application for end users. Use Grafana applications to observe and interact with the digital twin to help them optimize factory operations, increase production output, and improve equipment performance. Model Builder Allows you to create workspaces that will hold the resources, such as entity models and visual assets needed to create a digital twin. In this workspace, create entities that represent digital replicas of your equipment. Specify custom relationships between these entities to create a digital twin graph of your real-world system. Using the digital twin graph, customers can now issue geospatial queries such as finding all cameras that are pointing to an equipment to help with root cause analysis. Data Connector Associate entities with connectors (called as components in AWS IoT TwinMaker) to data stores such as AWS IoT SiteWise, to provide context to the data present in various data stores. Built-in Data Connectors for the following AWS services: AWS IoT SiteWise for equipment and time-series sensor data Amazon Kinesis Video Streams for video data Amazon Simple Storage Service (S3) for storage of visual resources (for example, CAD files) and data from business applications. Provides a framework for you to create your own Data Connectors to use with other data sources (such as Snowflake and Siemens MindSphere). Scene Composer A tool to create visualizations in 3D. You can bring previously built 3D/CAD models into your resource library in Amazon S3. these visual assets can be brought into a scene, and position the 3D assets to match your real-world systems. visual annotations such as tags on top of the base scene. References Introducing AWS IoT TwinMaker Further Reference aws-samples/aws-iot-twinmaker-samples</summary></entry><entry><title type="html">How to limit a process to run on One CPU core in Linux</title><link href="http://localhost:4000/linux/systems%20engineering/os/2021/12/06/How-to-run-a-process-on-a-particular-cpu-core.html" rel="alternate" type="text/html" title="How to limit a process to run on One CPU core in Linux" /><published>2021-12-06T12:33:22+05:30</published><updated>2021-12-06T12:33:22+05:30</updated><id>http://localhost:4000/linux/systems%20engineering/os/2021/12/06/How-to-run-a-process-on-a-particular-cpu-core</id><content type="html" xml:base="http://localhost:4000/linux/systems%20engineering/os/2021/12/06/How-to-run-a-process-on-a-particular-cpu-core.html">&lt;p&gt;This write-up explains different ways to restrict a program to run on a specific CPU core(s). This might be required for some high priority tasks/programs to run dedicatedly on few core on a multi-core/processor systems. This is used to achieve performance benefits from multi-processor systems.&lt;/p&gt;

&lt;h2 id=&quot;sched-setaffinity&quot;&gt;sched-setaffinity()&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Linux System Call&lt;/li&gt;
  &lt;li&gt;A process’s CPU &lt;strong&gt;&lt;em&gt;affinity mask&lt;/em&gt;&lt;/strong&gt; determines the &lt;em&gt;&lt;u&gt;set of CPUs on which it is eligible to run&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Dedicating one CPU to a particular process can be acheved as:
    &lt;ul&gt;
      &lt;li&gt;setting the affinity mask of that process to specify a single CPU.&lt;/li&gt;
      &lt;li&gt;and setting the affinity mask of all other processes to exclude that CPU.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Restricting a process to run on a single CPU also avoids the performance cost caused by the cache invalidation that occurs when a process ceases to execute on one CPU and then recommences execution on a different CPU&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;These restrictions on the actual set of CPUs on which the process will run are silently imposed by the kernel.&lt;/li&gt;
  &lt;li&gt;The affinity mask is actually a per-thread attribute that can be adjusted independently for each of the threads in a thread group.
    &lt;ul&gt;
      &lt;li&gt;use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-a&lt;/code&gt; option with &lt;strong&gt;&lt;em&gt;taskset&lt;/em&gt;&lt;/strong&gt; command to add affinity mask to all threads of the process.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A child created via fork(2) inherits its parent’s CPU affinity mask. The affinity mask is preserved across an execve(2).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;python-3-sched_setaffinity&quot;&gt;Python 3: sched_setaffinity()&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Python 3 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;os module&lt;/code&gt; supports sched-setaffinity() method&lt;/li&gt;
  &lt;li&gt;Example usage:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; import os
&amp;gt;&amp;gt;&amp;gt; os.sched_getaffinity(0)
{0, 1, 2, 3}
----------------

&amp;gt;&amp;gt;&amp;gt; os.sched_setaffinity(0, {1, 3})
&amp;gt;&amp;gt;&amp;gt; os.sched_getaffinity(0)
{1, 3}
----------------

&amp;gt;&amp;gt;&amp;gt; x = {i for i in range(10)}
&amp;gt;&amp;gt;&amp;gt; x
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
----------------

&amp;gt;&amp;gt;&amp;gt; os.sched_setaffinity(0, x)
&amp;gt;&amp;gt;&amp;gt; os.sched_getaffinity(0)
{0, 1, 2, 3}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;reference&quot;&gt;Reference&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://linux.die.net/man/2/sched_setaffinity&quot;&gt;sched_setaffinity(2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/dev/library/os.html#os.sched_setaffinity&quot;&gt;Python 3: sched_setaffinity()&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/15639779/why-does-multiprocessing-use-only-a-single-core-after-i-import-numpy&quot;&gt;Why does multiprocessing use only a single core after I import numpy?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;taskset&quot;&gt;taskset&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Linux Command&lt;/li&gt;
  &lt;li&gt;set or retrieve a process’s &lt;strong&gt;*&lt;u&gt;CPU affinity*&lt;/u&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Works on a running process and can also launch a new process.&lt;/li&gt;
  &lt;li&gt;CPU affinity is a scheduler property that “bonds” a process to a given set of CPUs on the system.&lt;/li&gt;
  &lt;li&gt;The Linux scheduler will honor the given CPU affinity and the process will not run on any other CPUs.&lt;/li&gt;
  &lt;li&gt;:warning: Note that the Linux scheduler also supports natural CPU affinity:
    &lt;ul&gt;
      &lt;li&gt;the scheduler attempts to keep processes on the same CPU as long as practical for performance reasons.&lt;/li&gt;
      &lt;li&gt;Therefore, forcing a specific CPU affinity is useful only in certain applications.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The CPU affinity is represented as a &lt;strong&gt;bitmask&lt;/strong&gt;,
    &lt;ul&gt;
      &lt;li&gt;with the lowest order bit corresponding to the first logical CPU.&lt;/li&gt;
      &lt;li&gt;and the highest order bit corresponding to the last logical CPU.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;:memo: Not all CPUs may exist on a given system but a mask may specify more CPUs than are present.&lt;/li&gt;
  &lt;li&gt;:memo: A retrieved mask will reflect only the bits that correspond to CPUs physically on the system.&lt;/li&gt;
  &lt;li&gt;Examples:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  taskset -c 0 mycommand --option    # start a command with the given affinity.
  -------------------

  taskset -c -pa 0 1234              # set the affinity of a running process.
                                     # -a for applying affinity mask to all the threads of the process.
  -------------------

  0x00000001
     is processor #0,
  -------------------

  0x00000003
     is processors #0 and #1,
  -------------------

  0xFFFFFFFF
     is processors #0 through #31,
  -------------------

  32
     is processors #1, #4, and #5,
  -------------------

  --cpu-list 0-2,6
  or
  - c 0-2,6
     is processors #0, #1, #2, and #6.
  -------------------

  --cpu-list 0-10:2
  or
  -c 0-10:2
     is processors #0, #2, #4, #6, #8 and #10. The suffix &quot;:N&quot;
     specifies stride in the range, for example 0-10:3 is
     interpreted as 0,3,6,9 list.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;reference-1&quot;&gt;Reference&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://man7.org/linux/man-pages/man1/taskset.1.html&quot;&gt;taskset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference-2&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://unix.stackexchange.com/questions/23106/how-to-limit-a-process-to-one-cpu-core-in-linux&quot;&gt;How to limit a process to one CPU core in Linux?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="Linux" /><category term="Systems Engineering" /><category term="OS" /><category term="Linux" /><category term="Systems Engineering" /><category term="OS" /><summary type="html">This write-up explains different ways to restrict a program to run on a specific CPU core(s). This might be required for some high priority tasks/programs to run dedicatedly on few core on a multi-core/processor systems. This is used to achieve performance benefits from multi-processor systems. sched-setaffinity() Linux System Call A process’s CPU affinity mask determines the set of CPUs on which it is eligible to run. Dedicating one CPU to a particular process can be acheved as: setting the affinity mask of that process to specify a single CPU. and setting the affinity mask of all other processes to exclude that CPU. Restricting a process to run on a single CPU also avoids the performance cost caused by the cache invalidation that occurs when a process ceases to execute on one CPU and then recommences execution on a different CPU. These restrictions on the actual set of CPUs on which the process will run are silently imposed by the kernel. The affinity mask is actually a per-thread attribute that can be adjusted independently for each of the threads in a thread group. use -a option with taskset command to add affinity mask to all threads of the process. A child created via fork(2) inherits its parent’s CPU affinity mask. The affinity mask is preserved across an execve(2). Python 3: sched_setaffinity() Python 3 os module supports sched-setaffinity() method Example usage: &amp;gt;&amp;gt;&amp;gt; import os &amp;gt;&amp;gt;&amp;gt; os.sched_getaffinity(0) {0, 1, 2, 3} ---------------- &amp;gt;&amp;gt;&amp;gt; os.sched_setaffinity(0, {1, 3}) &amp;gt;&amp;gt;&amp;gt; os.sched_getaffinity(0) {1, 3} ---------------- &amp;gt;&amp;gt;&amp;gt; x = {i for i in range(10)} &amp;gt;&amp;gt;&amp;gt; x {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} ---------------- &amp;gt;&amp;gt;&amp;gt; os.sched_setaffinity(0, x) &amp;gt;&amp;gt;&amp;gt; os.sched_getaffinity(0) {0, 1, 2, 3} Reference sched_setaffinity(2) Python 3: sched_setaffinity() Why does multiprocessing use only a single core after I import numpy? taskset Linux Command set or retrieve a process’s *CPU affinity*. Works on a running process and can also launch a new process. CPU affinity is a scheduler property that “bonds” a process to a given set of CPUs on the system. The Linux scheduler will honor the given CPU affinity and the process will not run on any other CPUs. :warning: Note that the Linux scheduler also supports natural CPU affinity: the scheduler attempts to keep processes on the same CPU as long as practical for performance reasons. Therefore, forcing a specific CPU affinity is useful only in certain applications. The CPU affinity is represented as a bitmask, with the lowest order bit corresponding to the first logical CPU. and the highest order bit corresponding to the last logical CPU. :memo: Not all CPUs may exist on a given system but a mask may specify more CPUs than are present. :memo: A retrieved mask will reflect only the bits that correspond to CPUs physically on the system. Examples: taskset -c 0 mycommand --option # start a command with the given affinity. ------------------- taskset -c -pa 0 1234 # set the affinity of a running process. # -a for applying affinity mask to all the threads of the process. ------------------- 0x00000001 is processor #0, ------------------- 0x00000003 is processors #0 and #1, ------------------- 0xFFFFFFFF is processors #0 through #31, ------------------- 32 is processors #1, #4, and #5, ------------------- --cpu-list 0-2,6 or - c 0-2,6 is processors #0, #1, #2, and #6. ------------------- --cpu-list 0-10:2 or -c 0-10:2 is processors #0, #2, #4, #6, #8 and #10. The suffix &quot;:N&quot; specifies stride in the range, for example 0-10:3 is interpreted as 0,3,6,9 list. Reference taskset Reference How to limit a process to one CPU core in Linux?</summary></entry><entry><title type="html">5G: Private 5G Networks for IoT Use cases</title><link href="http://localhost:4000/5g/iot/edge%20computing/aws%20iot/2021/12/04/Private-5G-Networks.html" rel="alternate" type="text/html" title="5G: Private 5G Networks for IoT Use cases" /><published>2021-12-04T12:33:22+05:30</published><updated>2021-12-04T12:33:22+05:30</updated><id>http://localhost:4000/5g/iot/edge%20computing/aws%20iot/2021/12/04/Private-5G-Networks</id><content type="html" xml:base="http://localhost:4000/5g/iot/edge%20computing/aws%20iot/2021/12/04/Private-5G-Networks.html">&lt;p&gt;This write-up explains different use cases of Private 5G Networks, specifically in IoT &amp;amp; Edge-Computing domains.&lt;/p&gt;

&lt;h2 id=&quot;problems-with-existing-enterprise-networks&quot;&gt;Problems with existing Enterprise Networks&lt;/h2&gt;
&lt;p&gt;Enterprise networks are under strain, because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Increasing internet connected-serving applications.&lt;/li&gt;
  &lt;li&gt;Increasing number of users.&lt;/li&gt;
  &lt;li&gt;Increasing number of Connected IoT Devices.&lt;/li&gt;
  &lt;li&gt;Increased Video content consumption.&lt;/li&gt;
  &lt;li&gt;Low Latency requirements.&lt;/li&gt;
  &lt;li&gt;Wired networks are inflexible and expensive to expand&lt;/li&gt;
  &lt;li&gt;Wi-Fi networks suffer from Coverage, Reliability &amp;amp; Capacity issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;advantages-of-5g-networks&quot;&gt;Advantages of 5G Networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Reliable network connectivity for use cases that involve device mobility.&lt;/li&gt;
  &lt;li&gt;Longer range than Wi-Fi networks.&lt;/li&gt;
  &lt;li&gt;Better coverage, especially in industrial/hospital environments.&lt;/li&gt;
  &lt;li&gt;Low-latency connectivity for IoT-Edge applications.&lt;/li&gt;
  &lt;li&gt;Full enterprise control over users, devices, network utilization and data. Implement enterprise-specific network configuration and security policies.&lt;/li&gt;
  &lt;li&gt;Convenience of CBRS (Citizens Broadband Radio Service) in the US with no need to acquire spectrum licenses.(US specific)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;private-5g-use-cases&quot;&gt;Private 5G use-cases&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Indoor Scenarios such as Industrial Manufacturing Hub or Hospitals or Retail stores
    &lt;ul&gt;
      &lt;li&gt;Improved coverage and reliability close to wired networks with flexibility as Wi-Fi.&lt;/li&gt;
      &lt;li&gt;Connect autonomous mobile robots (AMRs), automated guided vehicles (AGVs), video surveillance systems.&lt;/li&gt;
      &lt;li&gt;Underground deployments, where cellular network is often difficult to reach, unreliable.
        &lt;ul&gt;
          &lt;li&gt;Mining sites can be a great scenario for connectivity to Iot Sensors, video surveillance.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Point-of-sale (PoS) transactions, video surveillance in Retail stores.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Outdoor Scenarios such as Transport Hubs
    &lt;ul&gt;
      &lt;li&gt;Increase range and penetration over Wi-Fi in shipping ports, airports, train and bus terminals.&lt;/li&gt;
      &lt;li&gt;Reliable connectivity for IoT sensor data for predictive maintenance, video feeds for safety, push-to-talk (PTT) applications for communications.&lt;/li&gt;
      &lt;li&gt;Remote Places where public network services are not available like Oil-Gas, Mining IIoT sites&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;On-premise IoT-Edge-AI-ML Analysis
    &lt;ul&gt;
      &lt;li&gt;Low latency use-cases can be a reality as in case of Internet of Medical things (IoMT).&lt;/li&gt;
      &lt;li&gt;Real-time AI-ML diagnosis on sensors, video data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Large Images/Videos data analysis
    &lt;ul&gt;
      &lt;li&gt;As in case of Radiology scans or live video, or Raw image data, which are relatively large chunks of data.&lt;/li&gt;
      &lt;li&gt;with 5G low latency and dedicated network without the risk of channel interference, these loads can be transmitted in real-time and analysis be performed on it or it can be made available for remote monitoring or diagnosis services.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AR-VR applications
    &lt;ul&gt;
      &lt;li&gt;with all of the promises of 5G networks such as Low Latency, dedicated bandwidth, AR/VR application experiances can be made production grade, deployed and simultaneously used by large gatherings.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-private-5g&quot;&gt;AWS Private 5G&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;It simplifies the procurement, deployment, and installation process&lt;/li&gt;
  &lt;li&gt;Allowing customers to deploy their own 4G/LTE or 5G network.&lt;/li&gt;
  &lt;li&gt;In the US, AWS Private 5G uses CBRS (Citizens Broadband Radio Service), avoiding the need to procure spectrum licenses.&lt;/li&gt;
  &lt;li&gt;Integration with Spectrum Access System (SAS) for FCC regulation compliance are handled by AWS.&lt;/li&gt;
  &lt;li&gt;Delivers, provisions, and maintains all the preintegrated hardware, 5G Core and RAN software, and SIM cards.&lt;/li&gt;
  &lt;li&gt;Small cell radio units (RUs) are deployed on premises and the 5G Core (5GC) software runs either in an AWS Region or on premises using AWS managed infrastructure (AWS Outposts).&lt;/li&gt;
  &lt;li&gt;Setup Customized Bandwidths, Latency and Quality of Service (QoS) profiles for groups of devices and applications.&lt;/li&gt;
  &lt;li&gt;Pricing is based on Network Coverage and Capacity requirements, but not the number of connected devices.&lt;/li&gt;
  &lt;li&gt;Zero upfront charges for radio units, on-premises hardware, and SIMs. No installation charges or additional software licensing fees.&lt;/li&gt;
  &lt;li&gt;Provision a private cellular network of any size.&lt;/li&gt;
  &lt;li&gt;Expand coverage, and scale number of connected devices as needed.&lt;/li&gt;
  &lt;li&gt;AWS IAM integration to manage access between AWS services and devices in the network.
    &lt;ul&gt;
      &lt;li&gt;5G-powered SIM cards are treated as IAM resources.&lt;/li&gt;
      &lt;li&gt;Manage SIM control policies.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Amazon CloudWatch Monitoring
    &lt;ul&gt;
      &lt;li&gt;Query metrics for network status, connected APs or SIMs, uplink and downlink usage by user, device, network categories.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/private5g/&quot;&gt;AWS Private 5G&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="5G" /><category term="IoT" /><category term="Edge Computing" /><category term="AWS IoT" /><category term="5G" /><category term="IoT" /><category term="Edge Computing" /><category term="AWS IoT" /><summary type="html">This write-up explains different use cases of Private 5G Networks, specifically in IoT &amp;amp; Edge-Computing domains. Problems with existing Enterprise Networks Enterprise networks are under strain, because: Increasing internet connected-serving applications. Increasing number of users. Increasing number of Connected IoT Devices. Increased Video content consumption. Low Latency requirements. Wired networks are inflexible and expensive to expand Wi-Fi networks suffer from Coverage, Reliability &amp;amp; Capacity issues. Advantages of 5G Networks Reliable network connectivity for use cases that involve device mobility. Longer range than Wi-Fi networks. Better coverage, especially in industrial/hospital environments. Low-latency connectivity for IoT-Edge applications. Full enterprise control over users, devices, network utilization and data. Implement enterprise-specific network configuration and security policies. Convenience of CBRS (Citizens Broadband Radio Service) in the US with no need to acquire spectrum licenses.(US specific) Private 5G use-cases Indoor Scenarios such as Industrial Manufacturing Hub or Hospitals or Retail stores Improved coverage and reliability close to wired networks with flexibility as Wi-Fi. Connect autonomous mobile robots (AMRs), automated guided vehicles (AGVs), video surveillance systems. Underground deployments, where cellular network is often difficult to reach, unreliable. Mining sites can be a great scenario for connectivity to Iot Sensors, video surveillance. Point-of-sale (PoS) transactions, video surveillance in Retail stores. Outdoor Scenarios such as Transport Hubs Increase range and penetration over Wi-Fi in shipping ports, airports, train and bus terminals. Reliable connectivity for IoT sensor data for predictive maintenance, video feeds for safety, push-to-talk (PTT) applications for communications. Remote Places where public network services are not available like Oil-Gas, Mining IIoT sites On-premise IoT-Edge-AI-ML Analysis Low latency use-cases can be a reality as in case of Internet of Medical things (IoMT). Real-time AI-ML diagnosis on sensors, video data. Large Images/Videos data analysis As in case of Radiology scans or live video, or Raw image data, which are relatively large chunks of data. with 5G low latency and dedicated network without the risk of channel interference, these loads can be transmitted in real-time and analysis be performed on it or it can be made available for remote monitoring or diagnosis services. AR-VR applications with all of the promises of 5G networks such as Low Latency, dedicated bandwidth, AR/VR application experiances can be made production grade, deployed and simultaneously used by large gatherings. AWS Private 5G It simplifies the procurement, deployment, and installation process Allowing customers to deploy their own 4G/LTE or 5G network. In the US, AWS Private 5G uses CBRS (Citizens Broadband Radio Service), avoiding the need to procure spectrum licenses. Integration with Spectrum Access System (SAS) for FCC regulation compliance are handled by AWS. Delivers, provisions, and maintains all the preintegrated hardware, 5G Core and RAN software, and SIM cards. Small cell radio units (RUs) are deployed on premises and the 5G Core (5GC) software runs either in an AWS Region or on premises using AWS managed infrastructure (AWS Outposts). Setup Customized Bandwidths, Latency and Quality of Service (QoS) profiles for groups of devices and applications. Pricing is based on Network Coverage and Capacity requirements, but not the number of connected devices. Zero upfront charges for radio units, on-premises hardware, and SIMs. No installation charges or additional software licensing fees. Provision a private cellular network of any size. Expand coverage, and scale number of connected devices as needed. AWS IAM integration to manage access between AWS services and devices in the network. 5G-powered SIM cards are treated as IAM resources. Manage SIM control policies. Amazon CloudWatch Monitoring Query metrics for network status, connected APs or SIMs, uplink and downlink usage by user, device, network categories. Reference AWS Private 5G</summary></entry><entry><title type="html">AWS Greengrass Service V2: a Service to build, deploy and manage IoT applications on Edge Devices</title><link href="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/12/AWS-Greengrass.html" rel="alternate" type="text/html" title="AWS Greengrass Service V2: a Service to build, deploy and manage IoT applications on Edge Devices" /><published>2021-11-12T12:33:22+05:30</published><updated>2021-11-12T12:33:22+05:30</updated><id>http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/12/AWS-Greengrass</id><content type="html" xml:base="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/12/AWS-Greengrass.html">&lt;p&gt;This write-up will only focus on AWS Sagemaker ML service with respect to ML model deployment to Edge devices and Cloud.&lt;/p&gt;

&lt;h2 id=&quot;ml-inference-on-greengrass-devices&quot;&gt;ML Inference on Greengrass devices&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ML models can be trained using &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS Sagemaker&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; or custom ml trainining ways. Models are stored in &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS S3&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; for deployment to Greengrass devices.&lt;/li&gt;
  &lt;li&gt;ML models are deployed as artifacts in your components to perform inference on your core devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-components&quot;&gt;ML Components&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;AWS provides following Machine Learning components that can be deployed to edge devices to perform Machine Learning Inference.&lt;/li&gt;
  &lt;li&gt;ML models can be trained using &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS Sagemaker&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; or custom ml trainining ways. Models are stored in &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS S3&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; for deployment to Greengrass devices.&lt;/li&gt;
  &lt;li&gt;AWS-provided machine learning components are broadly categorized as follows:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Model component&lt;/em&gt;&lt;/strong&gt; — Contains machine learning models as &lt;em&gt;&lt;u&gt;Greengrass artifacts&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Runtime component&lt;/em&gt;&lt;/strong&gt; — Contains the &lt;em&gt;&lt;u&gt;script&lt;/u&gt;&lt;/em&gt; that installs the machine learning framework and its dependencies on the Greengrass core device.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Inference component&lt;/em&gt;&lt;/strong&gt; — Contains the &lt;em&gt;&lt;u&gt;inference code&lt;/u&gt;&lt;/em&gt; and includes &lt;em&gt;&lt;u&gt;component dependencies&lt;/u&gt;&lt;/em&gt; to install the machine learning framework and download pre-trained machine learning models.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;To perform custom machine learning inference with &lt;em&gt;&lt;u&gt;your own models&lt;/u&gt;&lt;/em&gt; that are stored in &lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon S3&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, or to use a &lt;em&gt;&lt;u&gt;different machine learning framework&lt;/u&gt;&lt;/em&gt;, you can use the recipes of the following public components as templates to create custom machine learning components.
    &lt;ul&gt;
      &lt;li&gt;Further Reference:
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/greengrass/v2/developerguide/ml-customization.html&quot;&gt;Customize your machine learning components&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AWS provided ML model components:
    &lt;ul&gt;
      &lt;li&gt;SageMaker Edge Manager&lt;/li&gt;
      &lt;li&gt;DLR image classification&lt;/li&gt;
      &lt;li&gt;DLR object detection&lt;/li&gt;
      &lt;li&gt;DLR image classification model store&lt;/li&gt;
      &lt;li&gt;DLR object detection model store&lt;/li&gt;
      &lt;li&gt;DLR installer&lt;/li&gt;
      &lt;li&gt;TensorFlow Lite image classification&lt;/li&gt;
      &lt;li&gt;TensorFlow Lite object detection&lt;/li&gt;
      &lt;li&gt;TensorFlow Lite image classification model store&lt;/li&gt;
      &lt;li&gt;TensorFlow Lite object detection model store&lt;/li&gt;
      &lt;li&gt;TensorFlow Lite installer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aws-sagemaker-edge-manager-agent&quot;&gt;AWS SageMaker Edge Manager agent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;With SageMaker Edge Manager, you can use Amazon SageMaker Neo-compiled models directly on your core device.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/on-premise%20cloud/cloud/aws/edge%20iot/ml/2021/11/11/AWS-SageMaker.html&quot;&gt;AWS SageMaker Edge Manager&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-references&quot;&gt;Further References&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/aws-samples/greengrass-v2-docker-ros-demo&quot;&gt;ROS2 Docker Sample Application with AWS IoT Greengrass 2.0&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><summary type="html">This write-up will only focus on AWS Sagemaker ML service with respect to ML model deployment to Edge devices and Cloud. ML Inference on Greengrass devices ML models can be trained using AWS Sagemaker or custom ml trainining ways. Models are stored in AWS S3 for deployment to Greengrass devices. ML models are deployed as artifacts in your components to perform inference on your core devices. ML Components AWS provides following Machine Learning components that can be deployed to edge devices to perform Machine Learning Inference. ML models can be trained using AWS Sagemaker or custom ml trainining ways. Models are stored in AWS S3 for deployment to Greengrass devices. AWS-provided machine learning components are broadly categorized as follows: Model component — Contains machine learning models as Greengrass artifacts. Runtime component — Contains the script that installs the machine learning framework and its dependencies on the Greengrass core device. Inference component — Contains the inference code and includes component dependencies to install the machine learning framework and download pre-trained machine learning models. To perform custom machine learning inference with your own models that are stored in Amazon S3, or to use a different machine learning framework, you can use the recipes of the following public components as templates to create custom machine learning components. Further Reference: Customize your machine learning components AWS provided ML model components: SageMaker Edge Manager DLR image classification DLR object detection DLR image classification model store DLR object detection model store DLR installer TensorFlow Lite image classification TensorFlow Lite object detection TensorFlow Lite image classification model store TensorFlow Lite object detection model store TensorFlow Lite installer AWS SageMaker Edge Manager agent With SageMaker Edge Manager, you can use Amazon SageMaker Neo-compiled models directly on your core device. AWS SageMaker Edge Manager Further References ROS2 Docker Sample Application with AWS IoT Greengrass 2.0</summary></entry><entry><title type="html">Why prefer Static memory allocation over Dynamic in Embedded Firmmware</title><link href="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/12/Why-Static-Memory-Allocation-Preferred-over-Dynamic-in-Embedded-Firmwares.html" rel="alternate" type="text/html" title="Why prefer Static memory allocation over Dynamic in Embedded Firmmware" /><published>2021-11-12T12:33:22+05:30</published><updated>2021-11-12T12:33:22+05:30</updated><id>http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/12/Why-Static-Memory-Allocation-Preferred-over-Dynamic-in-Embedded-Firmwares</id><content type="html" xml:base="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/12/Why-Static-Memory-Allocation-Preferred-over-Dynamic-in-Embedded-Firmwares.html">&lt;p&gt;This write-up will gather the pointers and conclude on why Static memory allocation is preferred over Dynamic memory in Embedded world.&lt;/p&gt;

&lt;h2 id=&quot;options-for-memory-allocation-pros--cons&quot;&gt;Options for Memory Allocation, Pros &amp;amp; Cons&lt;/h2&gt;
&lt;p&gt;Embedded systems have limited amount of Random Access Memory (RAM) which will be used across different types of memory sections like Stack, Heap, data, variables sections.&lt;/p&gt;

&lt;h3 id=&quot;static-memory-allocation-without-stack-or-heap&quot;&gt;Static memory allocation, without Stack or Heap&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;If all of the memory is allocated Statically, the the amount of RAM being used can be determined at Compile time.&lt;/li&gt;
  &lt;li&gt;This avoids the memory-related issues like: memory leaks, dangling pointers, segmentation fault, etc.&lt;/li&gt;
  &lt;li&gt;We have following options to allocate memory for all kind of data:
    &lt;ul&gt;
      &lt;li&gt;Global variables.&lt;/li&gt;
      &lt;li&gt;File Static variables.&lt;/li&gt;
      &lt;li&gt;Function Static Variables.&lt;/li&gt;
      &lt;li&gt;Local variables inside functions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;8-bit Microcontrollers like 8051 &amp;amp; PIC are usually designed to work with only Static memory.&lt;/li&gt;
  &lt;li&gt;If there is no support for stack in MCU hardware, all the local variables are stored in the same memory location and that memory location is not used by any other function, even if the function is not running.&lt;/li&gt;
  &lt;li&gt;This prohibits the use of recursion or any other mechanism that requires reentrant code.
    &lt;ul&gt;
      &lt;li&gt;e.g. an interrupt routine can’t call a function that may also be called by the main flow of execution.&lt;/li&gt;
      &lt;li&gt;but this guarantees, no memory related run-time issues.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;This provides robustness of the program at the cost of programming flexibility &amp;amp; efficiency.&lt;/li&gt;
  &lt;li&gt;This is only feasible for MCUs which are supposed to run a small system &amp;amp; where memory usage is very limited and can be predetermined exactly.&lt;/li&gt;
  &lt;li&gt;For larger systems, this approach of No Stack, No Heap is not feasible, as it will need enourmous amount of RAM for every individual memory allocation, which each need different memory location.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stack-based-memory-management&quot;&gt;Stack based memory management&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A block of memory is allocated on Stack for each invocation of a function and the same is regained after function exits.&lt;/li&gt;
  &lt;li&gt;It’s difficult for a recursive program to predict how much Stack size will be in worst case.&lt;/li&gt;
  &lt;li&gt;A multi-tasking system needs to have a separate Stack for each task.&lt;/li&gt;
  &lt;li&gt;Plus an extra on efor Interrupts.&lt;/li&gt;
  &lt;li&gt;Stack size is usually pre-determined and statically fixed.&lt;/li&gt;
  &lt;li&gt;Stack Overflow is an untimely issue in static stack size allocation, a task may run into it, which the stack of other task is empty at that time.&lt;/li&gt;
  &lt;li&gt;Best Practice:
    &lt;ul&gt;
      &lt;li&gt;Allocate the Stack Size to be 50% more than the worst case seen during testing.&lt;/li&gt;
      &lt;li&gt;Many RTOS systems provide a stack size tracing feature.&lt;/li&gt;
      &lt;li&gt;Check for infinite recursion cases, if any.&lt;/li&gt;
      &lt;li&gt;Check for local variables sizes, specially for large sizes, it may extend beyond the top of stack (stack overflow).&lt;/li&gt;
      &lt;li&gt;Check for large variable arrays &amp;amp; recursive functions, they determine the pattern of stack usage.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;heap-based-memory-management&quot;&gt;Heap based memory management&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Heap memory is assigned with malloc() and regained with free() in C.&lt;/li&gt;
  &lt;li&gt;Stack management, i.e. memory allocation and regaining is managed by compiler, but the onus of Heap management relies with the programmer.&lt;/li&gt;
  &lt;li&gt;Heap is a major reason of devious bugs in C programs.
    &lt;ul&gt;
      &lt;li&gt;You freed the memory with free(), but continue to access it with a pointer that is nor NULLED after free() and neither being checked for NULL. This is perfect receipe for untimely crash, unexpected behaviour.&lt;/li&gt;
      &lt;li&gt;You did not freed the memory for later access, but the pointers to that memory itself have gone out of scope. Now you are staring at Memory Leaks, the memory allocations that cannot be regained. It just a matter of time, you will run out of memory due to memory leak tending towards infinity with each call to that buggy code.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Memory Fragmentation:
    &lt;ul&gt;
      &lt;li&gt;This cannot be corrected at the application level.&lt;/li&gt;
      &lt;li&gt;Caused by the blocks of memory available being broken down into smaller pieces as many allocations and frees are performed.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=_G4HJDQjeP8&amp;amp;ab_channel=cpp4arduino&quot;&gt;Memory Heap Fragmentation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The conclusion is that heap use does involve an element of risk, which the programmer may choose to accept in return for a more flexible, RAM-efficient system.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;memory-pools&quot;&gt;Memory Pools&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Pools, or partitions, of fixed-size memory blocks can be used to completely eliminate the potential for fragmentation.&lt;/li&gt;
  &lt;li&gt;A compromise between static allocation and a general purpose heap.&lt;/li&gt;
  &lt;li&gt;This heap can be tuned at design time for the size of the requests that will be made, as Embedded programs are usually addressing and desined for a particular problem or use-case, not for general purposes.&lt;/li&gt;
  &lt;li&gt;Each pool contains an array of blocks. Unused blocks can be linked together in a list. The pools themselves are declared as arrays.&lt;/li&gt;
  &lt;li&gt;Size is fixed for each pool.&lt;/li&gt;
  &lt;li&gt;This system must be tuned by deciding which size blocks to make available and how many blocks to provide in each pool.&lt;/li&gt;
  &lt;li&gt;Defining pools at sizes which are powers of two (that is, 2, 4, 8, 16, 32, 64, etc.) is a good starting point.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/1725923/how-bad-is-it-to-use-dynamic-datastuctures-on-an-embedded-system&quot;&gt;how bad is it to use dynamic datastuctures on an embedded system?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.nongnu.org/avr-libc/user-manual/malloc.html&quot;&gt;Memory Areas and Using malloc()&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://barrgroup.com/embedded-systems/how-to/malloc-free-dynamic-memory-allocation&quot;&gt;How to Allocate Dynamic Memory Safely&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.embedded.com/use-malloc-why-not/&quot;&gt;Use malloc()? Why not?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/33430900/dynamic-memory-allocation-in-embedded-c&quot;&gt;Dynamic memory allocation in embedded C&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://open4tech.com/concept-heap-usage-embedded-systems/&quot;&gt;The Concept of Heap and Its Usage in Embedded Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.embeddedcomputing.com/technology/software-and-os/ides-application-programming/dynamic-memory-allocation-just-say-no&quot;&gt;Dynamic Memory Allocation - Just Say No&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/21370410/why-shouldnt-we-have-dynamic-allocated-memory-with-different-size-in-embedded-s&quot;&gt;Why shouldn’t we have dynamic allocated memory with different size in embedded system&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.embedded.com/mastering-stack-and-heap-for-system-reliability-part-1-calculating-stack-size/&quot;&gt;Mastering stack and heap for system reliability: Part 1 – Calculating stack size&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.embedded.com/dynamic-memory-and-heap-contiguity/&quot;&gt;Dynamic memory and heap contiguity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/37812732/use-of-malloc-in-embedded-c/37814174&quot;&gt;use of malloc in embedded c [closed]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><summary type="html">This write-up will gather the pointers and conclude on why Static memory allocation is preferred over Dynamic memory in Embedded world. Options for Memory Allocation, Pros &amp;amp; Cons Embedded systems have limited amount of Random Access Memory (RAM) which will be used across different types of memory sections like Stack, Heap, data, variables sections. Static memory allocation, without Stack or Heap If all of the memory is allocated Statically, the the amount of RAM being used can be determined at Compile time. This avoids the memory-related issues like: memory leaks, dangling pointers, segmentation fault, etc. We have following options to allocate memory for all kind of data: Global variables. File Static variables. Function Static Variables. Local variables inside functions. 8-bit Microcontrollers like 8051 &amp;amp; PIC are usually designed to work with only Static memory. If there is no support for stack in MCU hardware, all the local variables are stored in the same memory location and that memory location is not used by any other function, even if the function is not running. This prohibits the use of recursion or any other mechanism that requires reentrant code. e.g. an interrupt routine can’t call a function that may also be called by the main flow of execution. but this guarantees, no memory related run-time issues. This provides robustness of the program at the cost of programming flexibility &amp;amp; efficiency. This is only feasible for MCUs which are supposed to run a small system &amp;amp; where memory usage is very limited and can be predetermined exactly. For larger systems, this approach of No Stack, No Heap is not feasible, as it will need enourmous amount of RAM for every individual memory allocation, which each need different memory location. Stack based memory management A block of memory is allocated on Stack for each invocation of a function and the same is regained after function exits. It’s difficult for a recursive program to predict how much Stack size will be in worst case. A multi-tasking system needs to have a separate Stack for each task. Plus an extra on efor Interrupts. Stack size is usually pre-determined and statically fixed. Stack Overflow is an untimely issue in static stack size allocation, a task may run into it, which the stack of other task is empty at that time. Best Practice: Allocate the Stack Size to be 50% more than the worst case seen during testing. Many RTOS systems provide a stack size tracing feature. Check for infinite recursion cases, if any. Check for local variables sizes, specially for large sizes, it may extend beyond the top of stack (stack overflow). Check for large variable arrays &amp;amp; recursive functions, they determine the pattern of stack usage. Heap based memory management Heap memory is assigned with malloc() and regained with free() in C. Stack management, i.e. memory allocation and regaining is managed by compiler, but the onus of Heap management relies with the programmer. Heap is a major reason of devious bugs in C programs. You freed the memory with free(), but continue to access it with a pointer that is nor NULLED after free() and neither being checked for NULL. This is perfect receipe for untimely crash, unexpected behaviour. You did not freed the memory for later access, but the pointers to that memory itself have gone out of scope. Now you are staring at Memory Leaks, the memory allocations that cannot be regained. It just a matter of time, you will run out of memory due to memory leak tending towards infinity with each call to that buggy code. Memory Fragmentation: This cannot be corrected at the application level. Caused by the blocks of memory available being broken down into smaller pieces as many allocations and frees are performed. Memory Heap Fragmentation The conclusion is that heap use does involve an element of risk, which the programmer may choose to accept in return for a more flexible, RAM-efficient system. Memory Pools Pools, or partitions, of fixed-size memory blocks can be used to completely eliminate the potential for fragmentation. A compromise between static allocation and a general purpose heap. This heap can be tuned at design time for the size of the requests that will be made, as Embedded programs are usually addressing and desined for a particular problem or use-case, not for general purposes. Each pool contains an array of blocks. Unused blocks can be linked together in a list. The pools themselves are declared as arrays. Size is fixed for each pool. This system must be tuned by deciding which size blocks to make available and how many blocks to provide in each pool. Defining pools at sizes which are powers of two (that is, 2, 4, 8, 16, 32, 64, etc.) is a good starting point. References how bad is it to use dynamic datastuctures on an embedded system? Memory Areas and Using malloc() How to Allocate Dynamic Memory Safely Use malloc()? Why not? Dynamic memory allocation in embedded C The Concept of Heap and Its Usage in Embedded Systems Dynamic Memory Allocation - Just Say No Why shouldn’t we have dynamic allocated memory with different size in embedded system Mastering stack and heap for system reliability: Part 1 – Calculating stack size Dynamic memory and heap contiguity use of malloc in embedded c [closed]</summary></entry><entry><title type="html">AWS Sagemaker Machine Learning Service, Sagemaker Neo, Sagemaker Edge Manager</title><link href="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/ml/2021/11/11/AWS-SageMaker.html" rel="alternate" type="text/html" title="AWS Sagemaker Machine Learning Service, Sagemaker Neo, Sagemaker Edge Manager" /><published>2021-11-11T12:33:22+05:30</published><updated>2021-11-11T12:33:22+05:30</updated><id>http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/ml/2021/11/11/AWS-SageMaker</id><content type="html" xml:base="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/ml/2021/11/11/AWS-SageMaker.html">&lt;p&gt;This write-up will only focus on AWS Sagemaker ML service with respect to ML model deployment to Edge devices and Cloud.&lt;/p&gt;

&lt;h2 id=&quot;aws-sagemaker-neo&quot;&gt;AWS Sagemaker Neo&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Amazon SageMaker Neo enables developers to optimize machine learning (ML) models for inference on SageMaker in the cloud and supported devices at the edge for the specific underlying hardware.&lt;/li&gt;
  &lt;li&gt;Optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy.&lt;/li&gt;
  &lt;li&gt;Amazon SageMaker &lt;strong&gt;&lt;em&gt;&lt;u&gt;Neo runtime&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; is supported on &lt;em&gt;&lt;u&gt;Android&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;iOS&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Linux&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;Windows&lt;/u&gt;&lt;/em&gt; operating systems.&lt;/li&gt;
  &lt;li&gt;Sagemaker neo can optimize the ML model to run on &lt;em&gt;&lt;u&gt;target hardware platform&lt;/u&gt;&lt;/em&gt; of Edge devices based on processors from &lt;em&gt;&lt;u&gt;Ambarella&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Apple&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;ARM&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Intel&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;MediaTek&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Nvidia&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;NXP&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Qualcomm&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;RockChip&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Texas Instruments&lt;/u&gt;&lt;/em&gt;, or &lt;em&gt;&lt;u&gt;Xilinx&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Compiles it into an &lt;em&gt;&lt;u&gt;executable&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;For inference in the cloud, SageMaker Neo speeds up inference and saves cost by creating &lt;em&gt;&lt;u&gt;an inference optimized container&lt;/u&gt;&lt;/em&gt; that include &lt;em&gt;&lt;u&gt;MXNet&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;PyTorch&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;TensorFlow&lt;/u&gt;&lt;/em&gt; integrated with Neo runtime for SageMaker hosting.&lt;/li&gt;
  &lt;li&gt;Amazon SageMaker Neo supports optimization for a model from the framework-specific format of &lt;em&gt;&lt;u&gt;DarkNet&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Keras&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;MXNet&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;PyTorch&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;TensorFlow&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;TensorFlow-Lite&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;ONNX&lt;/u&gt;&lt;/em&gt;, or &lt;em&gt;&lt;u&gt;XGBoost&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Amazon SageMaker &lt;em&gt;&lt;u&gt;Neo runtime&lt;/u&gt;&lt;/em&gt; occupies 1MB of storage and 2MB of memory, which is many times smaller than the storage and memory footprint of a framework, while providing a simple common &lt;em&gt;&lt;u&gt;API to run a compiled model&lt;/u&gt;&lt;/em&gt; originating in any framework.&lt;/li&gt;
  &lt;li&gt;Amazon SageMaker Neo takes advantage of partner-provided &lt;u&gt;accelerator libraries&lt;/u&gt; to deliver the best available performance for a deep learning model on heterogeneous hardware platforms with a &lt;u&gt;hardware accelerator&lt;/u&gt; as well as a CPU. Acceleration libraries such as &lt;em&gt;&lt;u&gt;Ambarella CV Tools&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Nvidia Tensor RT&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;Texas Instruments TIDL&lt;/u&gt;&lt;/em&gt; each support a specific set of functions and operators. SageMaker Neo automatically partitions your model so that the part with operators supported by the accelerator can run on the accelerator while the rest of the model runs on the CPU.&lt;/li&gt;
  &lt;li&gt;Amazon SageMaker Neo now compiles models for Amazon SageMaker &lt;strong&gt;&lt;em&gt;&lt;u&gt;INF1 instance&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; targets. SageMaker hosting provides a managed service for inference on the INF1 instances, which are based on the &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS Inferentia chip&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/sagemaker/neo/&quot;&gt;Amazon SageMaker Neo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-sagemaker-edge-manager&quot;&gt;AWS Sagemaker Edge Manager&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AWS Sagemaker Edge Manager consists of a Service running in AWS cloud and an Agent running on Edge devices.&lt;/li&gt;
  &lt;li&gt;Sagemaker Edge Manager deploys a ML model &lt;em&gt;&lt;u&gt;optimized&lt;/u&gt;&lt;/em&gt; with &lt;em&gt;&lt;u&gt;SageMaker Neo&lt;/u&gt;&lt;/em&gt; automatically so you don’t need to have Neo runtime installed on your devices in order to take advantage of the model optimizations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;agent&quot;&gt;Agent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Use the agent to &lt;em&gt;&lt;u&gt;make predictions&lt;/u&gt;&lt;/em&gt; with models loaded onto your edge devices.&lt;/li&gt;
  &lt;li&gt;The agent also &lt;em&gt;&lt;u&gt;collects model metrics&lt;/u&gt; and *&lt;u&gt;captures data&lt;/u&gt;&lt;/em&gt; at specific intervals.&lt;/li&gt;
  &lt;li&gt;Sample data is stored in your &lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon S3&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; bucket.&lt;/li&gt;
  &lt;li&gt;2 methods of installing and deploying the Edge Manager agent onto your edge devices:
    &lt;ul&gt;
      &lt;li&gt;Download the &lt;em&gt;&lt;u&gt;agent as a binary&lt;/u&gt;&lt;/em&gt; from the Amazon S3 release bucket.&lt;/li&gt;
      &lt;li&gt;Use the &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS IoT Greengrass V2&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; console or the &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS CLI&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; to deploy aws.greengrass.SageMakerEdgeManager.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;monitoring-deployments-across-fleets&quot;&gt;Monitoring deployments across fleets&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SageMaker Edge Manager also &lt;em&gt;&lt;u&gt;collects prediction data and sends&lt;/u&gt;&lt;/em&gt; a sample of the data to the cloud for monitoring, labeling, and retraining.&lt;/li&gt;
  &lt;li&gt;All data can be viewed in the &lt;strong&gt;&lt;em&gt;&lt;u&gt;SageMaker Edge Manager dashboard&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; which reports on the operation of deployed models.&lt;/li&gt;
  &lt;li&gt;The dashboard is useful to understand the performance of models running on each device across your fleet, understand overall fleet health and identify problematic models and particular devices.&lt;/li&gt;
  &lt;li&gt;If quality declines are detected, you can quickly spot them in the dashboard and also configure alerts through &lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon CloudWatch&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;signed-and-verifiable-ml-deployments&quot;&gt;Signed and Verifiable ML deployments&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SageMaker Edge Manager also &lt;em&gt;&lt;u&gt;cryptographically signs your models&lt;/u&gt;&lt;/em&gt; so you can verify that it was not tampered with as it moves from the cloud to edge devices.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;integration-with-device-applications&quot;&gt;Integration with device applications&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SageMaker Edge Manager &lt;em&gt;&lt;u&gt;supports&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;gRPC&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, an open source &lt;em&gt;&lt;u&gt;remote procedure call&lt;/u&gt;&lt;/em&gt;, which allows you to integrate SageMaker Edge Manager with your existing edge applications through APIs in common programming languages, such as Android Java, C# / .NET, Dart, Go, Java, Kotlin/JVM, Node.js, Objective-C, PHP, Python, Ruby, and Web.&lt;/li&gt;
  &lt;li&gt;Manages models separately from the rest of the application, so that updates to the model and the application are independent.
    &lt;h3 id=&quot;multiple-ml-models-serve-on-edge-devices&quot;&gt;Multiple ML models serve on edge devices&lt;/h3&gt;
  &lt;/li&gt;
  &lt;li&gt;ML applications usually require hosting and running multiple models concurrently on a device.&lt;/li&gt;
  &lt;li&gt;SageMaker Edge Manager will soon allow you to write &lt;em&gt;simple application logic&lt;/em&gt; to send one or more queries (i.e. load/unload models, run inference) &lt;em&gt;&lt;u&gt;independently to multiple models&lt;/u&gt;&lt;/em&gt; and &lt;em&gt;&lt;u&gt;rebalance hardware resource utilization&lt;/u&gt;&lt;/em&gt; when you add or update a model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;model-registry-and-lifecycle&quot;&gt;Model Registry and Lifecycle&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;SageMaker Edge Manager will soon be able to automate the build-train-deploy workflow from cloud to edge devices in Amazon SageMaker Edge Manager, and trace the lifecycle of each model.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=zS0Q3bdsLiU&amp;amp;t=3s&amp;amp;ab_channel=AmazonWebServices&quot;&gt;Sagemaker Edge Manager YouTube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html&quot;&gt;Developer Guide: SageMaker Edge Manager&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/sagemaker/latest/dg/edge-device-fleet-about.html&quot;&gt;Developer Guide: Edge Manager Agent&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_edge_manager/sagemaker_edge_example/sagemaker_edge_example.html?highlight=edge&quot;&gt;SageMaker Edge Manager Example&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-outpost-and-aws-sagemaker-edge-manager&quot;&gt;AWS Outpost and AWS Sagemaker Edge Manager&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/machine-learning/machine-learning-at-the-edge-with-aws-outposts-and-amazon-sagemaker/&quot;&gt;Machine Learning at the Edge with AWS Outposts and Amazon SageMaker&lt;/a&gt;
  &lt;img src=&quot;/assets/images/aws/aws-outpost-sagemaker-edge-manager.png&quot; alt=&quot;aws-outpost-sagemaker-edge-manager&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><category term="ML" /><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><category term="ML" /><summary type="html">This write-up will only focus on AWS Sagemaker ML service with respect to ML model deployment to Edge devices and Cloud. AWS Sagemaker Neo Amazon SageMaker Neo enables developers to optimize machine learning (ML) models for inference on SageMaker in the cloud and supported devices at the edge for the specific underlying hardware. Optimizes machine learning models for inference on cloud instances and edge devices to run faster with no loss in accuracy. Amazon SageMaker Neo runtime is supported on Android, iOS, Linux, and Windows operating systems. Sagemaker neo can optimize the ML model to run on target hardware platform of Edge devices based on processors from Ambarella, Apple, ARM, Intel, MediaTek, Nvidia, NXP, Qualcomm, RockChip, Texas Instruments, or Xilinx. Compiles it into an executable. For inference in the cloud, SageMaker Neo speeds up inference and saves cost by creating an inference optimized container that include MXNet, PyTorch, and TensorFlow integrated with Neo runtime for SageMaker hosting. Amazon SageMaker Neo supports optimization for a model from the framework-specific format of DarkNet, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, ONNX, or XGBoost. Amazon SageMaker Neo runtime occupies 1MB of storage and 2MB of memory, which is many times smaller than the storage and memory footprint of a framework, while providing a simple common API to run a compiled model originating in any framework. Amazon SageMaker Neo takes advantage of partner-provided accelerator libraries to deliver the best available performance for a deep learning model on heterogeneous hardware platforms with a hardware accelerator as well as a CPU. Acceleration libraries such as Ambarella CV Tools, Nvidia Tensor RT, and Texas Instruments TIDL each support a specific set of functions and operators. SageMaker Neo automatically partitions your model so that the part with operators supported by the accelerator can run on the accelerator while the rest of the model runs on the CPU. Amazon SageMaker Neo now compiles models for Amazon SageMaker INF1 instance targets. SageMaker hosting provides a managed service for inference on the INF1 instances, which are based on the AWS Inferentia chip. References Amazon SageMaker Neo AWS Sagemaker Edge Manager AWS Sagemaker Edge Manager consists of a Service running in AWS cloud and an Agent running on Edge devices. Sagemaker Edge Manager deploys a ML model optimized with SageMaker Neo automatically so you don’t need to have Neo runtime installed on your devices in order to take advantage of the model optimizations. Agent Use the agent to make predictions with models loaded onto your edge devices. The agent also collects model metrics and *captures data at specific intervals. Sample data is stored in your Amazon S3 bucket. 2 methods of installing and deploying the Edge Manager agent onto your edge devices: Download the agent as a binary from the Amazon S3 release bucket. Use the AWS IoT Greengrass V2 console or the AWS CLI to deploy aws.greengrass.SageMakerEdgeManager. Monitoring deployments across fleets SageMaker Edge Manager also collects prediction data and sends a sample of the data to the cloud for monitoring, labeling, and retraining. All data can be viewed in the SageMaker Edge Manager dashboard which reports on the operation of deployed models. The dashboard is useful to understand the performance of models running on each device across your fleet, understand overall fleet health and identify problematic models and particular devices. If quality declines are detected, you can quickly spot them in the dashboard and also configure alerts through Amazon CloudWatch. Signed and Verifiable ML deployments SageMaker Edge Manager also cryptographically signs your models so you can verify that it was not tampered with as it moves from the cloud to edge devices. Integration with device applications SageMaker Edge Manager supports gRPC, an open source remote procedure call, which allows you to integrate SageMaker Edge Manager with your existing edge applications through APIs in common programming languages, such as Android Java, C# / .NET, Dart, Go, Java, Kotlin/JVM, Node.js, Objective-C, PHP, Python, Ruby, and Web. Manages models separately from the rest of the application, so that updates to the model and the application are independent. Multiple ML models serve on edge devices ML applications usually require hosting and running multiple models concurrently on a device. SageMaker Edge Manager will soon allow you to write simple application logic to send one or more queries (i.e. load/unload models, run inference) independently to multiple models and rebalance hardware resource utilization when you add or update a model. Model Registry and Lifecycle SageMaker Edge Manager will soon be able to automate the build-train-deploy workflow from cloud to edge devices in Amazon SageMaker Edge Manager, and trace the lifecycle of each model. Reference Sagemaker Edge Manager YouTube Developer Guide: SageMaker Edge Manager Developer Guide: Edge Manager Agent SageMaker Edge Manager Example AWS Outpost and AWS Sagemaker Edge Manager Machine Learning at the Edge with AWS Outposts and Amazon SageMaker</summary></entry><entry><title type="html">AWS cloud services extended to on-premise data centres with AWS Outpost</title><link href="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/10/AWS-Outpost.html" rel="alternate" type="text/html" title="AWS cloud services extended to on-premise data centres with AWS Outpost" /><published>2021-11-10T12:33:22+05:30</published><updated>2021-11-10T12:33:22+05:30</updated><id>http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/10/AWS-Outpost</id><content type="html" xml:base="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/10/AWS-Outpost.html">&lt;h1 id=&quot;aws-outpost&quot;&gt;AWS Outpost&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;AWS Outposts is a &lt;em&gt;&lt;u&gt;fully managed service&lt;/u&gt;&lt;/em&gt; that extends same AWS infrastructure, AWS services, APIs, and tools to virtually any datacenter or &lt;em&gt;&lt;u&gt;on-premises&lt;/u&gt;&lt;/em&gt; facility for a consistent hybrid experience.&lt;/li&gt;
  &lt;li&gt;Ideal for workloads that require &lt;strong&gt;&lt;em&gt;&lt;u&gt;low latency&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; access to on-premises systems, &lt;strong&gt;&lt;em&gt;&lt;u&gt;local data processing&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, or &lt;strong&gt;&lt;em&gt;&lt;u&gt;local data storage&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;AWS compute, storage, database, and other services run locally on Outposts, and you can access the full range of AWS services available in the Region to build, manage, and scale your on-premises applications using familiar AWS services and tools.&lt;/li&gt;
  &lt;li&gt;Outposts are connected to the nearest AWS Region.&lt;/li&gt;
  &lt;li&gt;We can run following AWS Resources on AWS Outposts on premises:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Amazon EC2 instances&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon EBS volumes&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon EKS nodes&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon ECS clusters&lt;/strong&gt; (container-based services)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon RDS DB&lt;/strong&gt; instances (database services)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon EMR clusters&lt;/strong&gt; (analytics services)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Amazon S3&lt;/strong&gt; for AWS Outposts will be available in 2020 for local object storage on Outposts.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AWS Outposts allows you to securely store and process customer data that needs to remain on premises or in countries where there is no AWS region.&lt;/li&gt;
  &lt;li&gt;With AWS Outposts, we do not need to manage different APIs, manual software updates, and purchase of third-party hardware and support.&lt;/li&gt;
  &lt;li&gt;AWS Outposts is fully managed and supported by AWS. &lt;em&gt;&lt;u&gt;Outpost is delivered, installed, monitored, patched, and updated by AWS&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;AWS Outposts address low latency application requirements and local data processing requirements.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-outposts-compute--storage&quot;&gt;AWS Outposts Compute &amp;amp; Storage&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Choose from pre-validated configurations with mix of EC2, EBS, S3 capacity.&lt;/li&gt;
  &lt;li&gt;Or contact AWS to customize configurations to meet your needs.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;compute&quot;&gt;Compute&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;General purpose&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (M5/M5d):
    &lt;ul&gt;
      &lt;li&gt;a balance of compute, memory, and network resources&lt;/li&gt;
      &lt;li&gt;can be used for general-purpose workloads, &lt;em&gt;&lt;u&gt;web and application servers&lt;/u&gt;, *&lt;u&gt;backend servers for enterprise applications&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;gaming servers&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;caching fleets&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Compute optimized&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (C5/C5d):
    &lt;ul&gt;
      &lt;li&gt;suited for compute intensive applications such as &lt;em&gt;&lt;u&gt;batch processing&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;media transcoding&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;high performance web servers&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;high performance computing (HPC)&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;scientific modeling&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;dedicated gaming servers&lt;/u&gt;&lt;/em&gt; and &lt;em&gt;&lt;u&gt;ad server engines&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;machine learning inference&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Memory optimized&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (R5/R5d):
    &lt;ul&gt;
      &lt;li&gt;to deliver fast performance for workloads that &lt;em&gt;&lt;u&gt;process large data sets&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;in memory&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;suited for memory intensive applications such as &lt;em&gt;&lt;u&gt;high-performance databases&lt;/u&gt;&lt;/em&gt;, distributed web scale &lt;em&gt;&lt;u&gt;in-memory caches&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;mid-size in-memory databases&lt;/u&gt;&lt;/em&gt;, real- time &lt;em&gt;&lt;u&gt;big data analytics&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Graphics optimized&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (G4dn):
    &lt;ul&gt;
      &lt;li&gt;To accelerate &lt;em&gt;&lt;u&gt;machine learning inference&lt;/u&gt;&lt;/em&gt;.
        &lt;ul&gt;
          &lt;li&gt;For machine learning inference for applications like &lt;em&gt;&lt;u&gt;adding metadata to an image&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;object detection&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;recommender systems&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;automated speech recognition&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;language translation&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;em&gt;&lt;u&gt;graphics-intensive workloads&lt;/u&gt;&lt;/em&gt;:
        &lt;ul&gt;
          &lt;li&gt;For building and running graphics-intensive applications, such as &lt;em&gt;&lt;u&gt;remote graphics workstations&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;video transcoding&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;photo-realistic design&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;game streaming&lt;/u&gt;&lt;/em&gt; in the cloud.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;I/O optimized&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (I3en):
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Non-Volatile Memory Express (NVMe) SSD&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; instance storage optimized for &lt;em&gt;&lt;u&gt;low latency&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;high random I/O performance&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;high sequential disk throughput&lt;/u&gt;&lt;/em&gt;, and offers the lowest price per GB of SSD instance storage on Amazon EC2.&lt;/li&gt;
      &lt;li&gt;Suited for &lt;em&gt;&lt;u&gt;NoSQL databases&lt;/u&gt;&lt;/em&gt; (Cassandra, MongoDB, Redis), &lt;em&gt;&lt;u&gt;in-memory databases&lt;/u&gt;&lt;/em&gt; (Aerospike), &lt;em&gt;&lt;u&gt;scale-out transactional databases&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;distributed file systems&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;data warehousing&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Elasticsearch&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;analytics workloads&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;storage&quot;&gt;Storage&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon EBS&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;local instance storage&lt;/em&gt;&lt;/strong&gt; - that gets vanished when EC2 instance is stopped.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Elastic Block Store (EBS)&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;&lt;u&gt;gp2 volumes&lt;/u&gt;&lt;/em&gt; for &lt;em&gt;&lt;u&gt;persistent&lt;/u&gt;&lt;/em&gt; block storage.&lt;/li&gt;
      &lt;li&gt;snapshot and restore capabilities&lt;/li&gt;
      &lt;li&gt;lets you &lt;a href=&quot;/_posts/2021-06-30-Extend-EC2-EBS-Volume-size-without-downtime.md&quot;&gt;increase volume size without any performance impact&lt;/a&gt;.&lt;/li&gt;
      &lt;li&gt;All EBS volumes and snapshots on Outposts are &lt;em&gt;&lt;u&gt;fully encrypted&lt;/u&gt;&lt;/em&gt; by default.&lt;/li&gt;
      &lt;li&gt;EBS is offered in tiers of 11 TB, 33 TB, and 55 TB.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon S3&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Store, Retrieve Data on Outpost&lt;/li&gt;
      &lt;li&gt;Secure Data&lt;/li&gt;
      &lt;li&gt;Control Access&lt;/li&gt;
      &lt;li&gt;Tags, Reports&lt;/li&gt;
      &lt;li&gt;S3 APIs&lt;/li&gt;
      &lt;li&gt;Add 26 TB, 48 TB, 96 TB, 240 TB, or 380 TB of S3 storage capacity&lt;/li&gt;
      &lt;li&gt;Create &lt;em&gt;&lt;u&gt;up to 100 buckets per AWS account&lt;/u&gt;&lt;/em&gt; on each Outpost&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon EBS Snapshot&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;a point-in-time copy of your EBS volumes.&lt;/li&gt;
      &lt;li&gt;Snapshots of EBS volumes on your Outpost are stored on Amazon S3 &lt;em&gt;&lt;u&gt;in the Region&lt;/u&gt;&lt;/em&gt;
        &lt;ul&gt;
          &lt;li&gt;in the region means in nearest cloud region? and not on local Outpost S3 storage?
            &lt;ul&gt;
              &lt;li&gt;If you have S3 provisioned on your Outpost, then you can store EBS snapshot on Outpost S3 itself, it’s called &lt;em&gt;&lt;u&gt;EBS Local Snapshot&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;use EBS Local Snapshots on Outposts for &lt;strong&gt;&lt;em&gt;&lt;u&gt;disaster recovery&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; and &lt;strong&gt;&lt;em&gt;&lt;u&gt;back up&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
      &lt;li&gt;Secure and protect data on EBS storage using &lt;em&gt;&lt;u&gt;resource-level&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;IAM policies&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;migration&quot;&gt;Migration&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/cloudendure-migration/&quot;&gt;CloudEndure Migration&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;allows customers to migrate workloads onto AWS Outposts from physical, virtual, or cloud-based sources, from on-premises locations, public AWS Regions, and other clouds to Outposts.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;using EBS Local Snapshots on Outposts:
    &lt;ul&gt;
      &lt;li&gt;migrate workloads from any source directly onto Outposts, or from one Outpost to another, without requiring the EBS snapshot data to go through the region.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/cloudendure-disaster-recovery/&quot;&gt;CloudEndure Disaster Recovery&lt;/a&gt;:
    &lt;ul&gt;
      &lt;li&gt;business continuity solution for physical, virtual, and cloud-based workloads onto AWS Outposts.&lt;/li&gt;
      &lt;li&gt;you can replicate and recover:
        &lt;ul&gt;
          &lt;li&gt;from on-premises to Outposts&lt;/li&gt;
          &lt;li&gt;from AWS Regions onto Outposts&lt;/li&gt;
          &lt;li&gt;from Outposts into AWS Regions&lt;/li&gt;
          &lt;li&gt;and between two Outposts.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;CloudEndure Disaster Recovery improves resilience, enabling &lt;em&gt;&lt;u&gt;recovery point objectives (RPOs)&lt;/u&gt;&lt;/em&gt; of seconds and &lt;em&gt;&lt;u&gt;recovery time objectives&lt;/u&gt;&lt;/em&gt; (RTOs) of minutes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;networking&quot;&gt;Networking&lt;/h2&gt;
&lt;h3 id=&quot;extended-vpc&quot;&gt;Extended VPC&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Extend your existing &lt;strong&gt;&lt;em&gt;Amazon VPC&lt;/em&gt;&lt;/strong&gt; to your Outpost in your on premises location.&lt;/li&gt;
  &lt;li&gt;Create a &lt;strong&gt;&lt;em&gt;subnet&lt;/em&gt;&lt;/strong&gt; in your &lt;em&gt;&lt;u&gt;regional VPC&lt;/u&gt;&lt;/em&gt; and associate it with an Outpost just as you associate subnets with an Availability Zone in an AWS Region.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Instances in Outpost subnets communicate with other instances in the AWS Region using&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;private IP addresses&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, all &lt;em&gt;&lt;u&gt;within the same VPC&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;local-gateway&quot;&gt;Local Gateway&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Each Outpost provides a new &lt;strong&gt;&lt;em&gt;local gateway (LGW)&lt;/em&gt;&lt;/strong&gt; that allows you to &lt;em&gt;&lt;u&gt;connect your Outpost resources with your on premises networks&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;LGW enables &lt;em&gt;&lt;u&gt;low latency connectivity&lt;/u&gt;&lt;/em&gt; between the Outpost and any local data sources, end users, local machinery and equipment, or local databases.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;load-balancer&quot;&gt;Load Balancer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;You can provision an &lt;strong&gt;&lt;em&gt;&lt;u&gt;Application Load Balancer (ALB)&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; to automatically &lt;em&gt;&lt;u&gt;distribute incoming HTTP(S) traffic&lt;/u&gt;&lt;/em&gt; across &lt;em&gt;&lt;u&gt;multiple targets&lt;/u&gt;&lt;/em&gt; on your Outposts, such as &lt;em&gt;&lt;u&gt;Amazon EC2 instances, containers, and IP addresses&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;ALB on Outposts is &lt;em&gt;&lt;u&gt;fully managed&lt;/u&gt;&lt;/em&gt;, operates in a &lt;em&gt;&lt;u&gt;single subnet&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;scales automatically&lt;/u&gt;&lt;/em&gt; up to the capacity available on the Outposts rack to meet varying levels of application load without manual intervention.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;private-connection-to-aws-cloud&quot;&gt;Private Connection to AWS Cloud&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;With AWS Outposts Private Connectivity, you can establish a service link &lt;strong&gt;&lt;em&gt;&lt;u&gt;VPN connection&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; &amp;lt;/u&amp;gt;from your Outposts to the AWS Region over&amp;lt;/u&amp;gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS Direct Connect&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Minimizes public internet exposure&lt;/li&gt;
  &lt;li&gt;Removes the need for special firewall configurations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-services-on-outposts&quot;&gt;AWS Services on Outposts:&lt;/h2&gt;
&lt;h3 id=&quot;containers&quot;&gt;Containers&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon ECS&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Scalable, High-performance &lt;em&gt;&lt;u&gt;container orchestration service&lt;/u&gt;&lt;/em&gt;, supports &lt;em&gt;&lt;u&gt;Docker containers&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;run and scale containerized applications.&lt;/li&gt;
      &lt;li&gt;ECS eliminates the need for:
        &lt;ul&gt;
          &lt;li&gt;Install and Operate your own container orchestration software&lt;/li&gt;
          &lt;li&gt;Manage and Scale a cluster of virtual machines&lt;/li&gt;
          &lt;li&gt;Schedule containers on those virtual machines&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;With simple &lt;em&gt;&lt;u&gt;API calls&lt;/u&gt;&lt;/em&gt;, you can &lt;u&gt;launch and stop&lt;/u&gt; Docker-enabled applications and &lt;em&gt;&lt;u&gt;query the complete state of your application&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon EKS&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Managed Service to run Kubernetes.&lt;/li&gt;
      &lt;li&gt;Used to run Containerized applications.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;AWS ECS vs AWS EKS&lt;/u&gt;&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;/assets/docs/Amazon-ECS-vs-Amazon-EKS.pdf&quot;&gt;Amazon ECS vs Amazon EKS: making sense of AWS container services&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;databases&quot;&gt;Databases&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon RDS (Relational Database Service) &lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;supports &lt;em&gt;&lt;u&gt;Microsoft SQL Server&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;MySQL&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;PostgreSQL&lt;/u&gt;&lt;/em&gt; database engines.&lt;/li&gt;
      &lt;li&gt;Amazon RDS provides cost-efficient and resizable capacity while automating time-consuming administration tasks including infrastructure provisioning, database setup, patching, and backups.&lt;/li&gt;
      &lt;li&gt;fully managed databases on premises&lt;/li&gt;
      &lt;li&gt;Amazon RDS can be managed using AWS Management Console, APIs, and CLI as if in cloud.&lt;/li&gt;
      &lt;li&gt;Amazon RDS enables low-cost, high-availability hybrid deployments, with disaster recovery back to the AWS Region, read replica bursting to Amazon RDS in the cloud, and long-term archival in Amazon Simple Storage Service (Amazon S3) in the cloud.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon ElastiCache&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Fully managed &lt;em&gt;&lt;u&gt;in-memory data store&lt;/u&gt;&lt;/em&gt;, compatible with &lt;em&gt;&lt;u&gt;Redis&lt;/u&gt;&lt;/em&gt; or &lt;em&gt;&lt;u&gt;Memcached&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;Optimized for real-time applications with &lt;em&gt;&lt;u&gt;sub-millisecond latency&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Amazon ElastiCache on Outposts enables real-time use cases like &lt;em&gt;&lt;u&gt;Caching&lt;/u&gt;&lt;/em&gt;, &lt;em&gt;&lt;u&gt;Session Stores&lt;/u&gt;&lt;/em&gt;, Gaming, Geospatial Services, &lt;em&gt;&lt;u&gt;Real-Time Analytics&lt;/u&gt;&lt;/em&gt;, and &lt;em&gt;&lt;u&gt;Queuing&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-analytics&quot;&gt;Data Analytics&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Amazon EMR&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Deploys secure and managed EMR clusters.&lt;/li&gt;
      &lt;li&gt;Deploys latest versions of &lt;strong&gt;&lt;em&gt;&lt;u&gt;Apache Spark&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;&lt;u&gt;Apache Hive&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, and &lt;strong&gt;&lt;em&gt;&lt;u&gt;Presto&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; to access critical on premises data sources and systems for big data analytics.&lt;/li&gt;
      &lt;li&gt;Use the EMR console, SDK, or CLI to specify the subnet associated with your Outpost to launch EMR clusters.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;upgrades-to-outpost-services&quot;&gt;Upgrades to Outpost Services&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;AWS services running locally on Outposts will be upgraded automatically to the latest version as and when available.&lt;/li&gt;
  &lt;li&gt;Amazon RDS like services also patch both OS and database engines within scheduled maintenance windows with minimum downtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;access-to-cloud-hosted-regional-services&quot;&gt;Access to Cloud hosted Regional Services&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;We can &lt;em&gt;&lt;u&gt;extend&lt;/u&gt;&lt;/em&gt; our &lt;strong&gt;&lt;em&gt;Amazon Virtual Private Cloud(VPC)&lt;/em&gt;&lt;/strong&gt; on premises and &lt;em&gt;&lt;u&gt;run some AWS services locally on Outposts and also connect to a broad range of services available in the local AWS Region&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;We can access all regional AWS services in your private VPC environment, for example, through Interface Endpoints, Gateway Endpoints, or their regional public endpoints.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-tools&quot;&gt;AWS Tools:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;With AWS Outposts, customers &lt;em&gt;&lt;u&gt;can access AWS tools&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;running in the region&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; such as, &lt;em&gt;&lt;u&gt;AWS CloudFormation, Amazon CloudWatch, AWS CloudTrail, Elastic BeanStalk, Cloud 9,&lt;/u&gt;&lt;/em&gt; and others to run and &lt;em&gt;&lt;u&gt;manage workloads&lt;/u&gt;&lt;/em&gt; on AWS Outposts the same way they do in the cloud.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;aws-resource-access-manager&quot;&gt;AWS Resource Access Manager&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;AWS Resource Access Manager (RAM)&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; lets customers share access to Outposts resources – EC2 instances, EBS volumes, S3 capacity, subnets, and local gateways (LGWs) – across multiple accounts under the same AWS organization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;further-readings&quot;&gt;Further Readings&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://d1.awsstatic.com/Solutions/Outposts%20Manufacturing%20Solution%20Brief%20US%20Letter%20AWS%2009.30.20%20FINAL.pdf&quot;&gt;AWS Outpost with AWS Greengrass Sample Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://d1.awsstatic.com/re19/AWS19-0029%20Outpost%20Solution%20Brief_v7%20Final.pdf&quot;&gt;AWS Outpost Healthcare Sample Architecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://d1.awsstatic.com/product-marketing/Outposts/AWS%20HCLS%20eBook.pdf&quot;&gt;Cloud at the Edge for Healthcare and Life Sciences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yet To Read:&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/containers/deploying-containerized-application-on-aws-outposts-with-amazon-eks/&quot;&gt;Deploying Containerized Application on AWS Outposts with Amazon EKS&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/compute/building-modern-applications-with-amazon-eks-on-amazon-outposts/&quot;&gt;Building Modern Applications with Amazon EKS on Amazon Outposts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/networking-and-content-delivery/configuring-an-application-load-balancer-on-aws-outposts/&quot;&gt;Configuring an Application Load Balancer on AWS Outposts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/compute/managing-your-aws-outposts-capacity-using-amazon-cloudwatch-and-aws-lambda/&quot;&gt;Managing your AWS Outposts capacity using Amazon CloudWatch and AWS Lambda&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/storage/run-applications-on-premises-and-access-s3-objects-from-aws-outposts/&quot;&gt;Run applications on premises and access Amazon S3 objects from AWS Outposts&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-outposts-private-connectivity/&quot;&gt;Introducing AWS Outposts private connectivity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><summary type="html">AWS Outpost AWS Outposts is a fully managed service that extends same AWS infrastructure, AWS services, APIs, and tools to virtually any datacenter or on-premises facility for a consistent hybrid experience. Ideal for workloads that require low latency access to on-premises systems, local data processing, or local data storage. AWS compute, storage, database, and other services run locally on Outposts, and you can access the full range of AWS services available in the Region to build, manage, and scale your on-premises applications using familiar AWS services and tools. Outposts are connected to the nearest AWS Region. We can run following AWS Resources on AWS Outposts on premises: Amazon EC2 instances Amazon EBS volumes Amazon EKS nodes Amazon ECS clusters (container-based services) Amazon RDS DB instances (database services) Amazon EMR clusters (analytics services) Amazon S3 for AWS Outposts will be available in 2020 for local object storage on Outposts. AWS Outposts allows you to securely store and process customer data that needs to remain on premises or in countries where there is no AWS region. With AWS Outposts, we do not need to manage different APIs, manual software updates, and purchase of third-party hardware and support. AWS Outposts is fully managed and supported by AWS. Outpost is delivered, installed, monitored, patched, and updated by AWS. AWS Outposts address low latency application requirements and local data processing requirements. AWS Outposts Compute &amp;amp; Storage Choose from pre-validated configurations with mix of EC2, EBS, S3 capacity. Or contact AWS to customize configurations to meet your needs. Compute General purpose (M5/M5d): a balance of compute, memory, and network resources can be used for general-purpose workloads, web and application servers, *backend servers for enterprise applications, gaming servers, and caching fleets. Compute optimized (C5/C5d): suited for compute intensive applications such as batch processing, media transcoding, high performance web servers, high performance computing (HPC), scientific modeling, dedicated gaming servers and ad server engines, machine learning inference. Memory optimized (R5/R5d): to deliver fast performance for workloads that process large data sets in memory. suited for memory intensive applications such as high-performance databases, distributed web scale in-memory caches, mid-size in-memory databases, real- time big data analytics. Graphics optimized (G4dn): To accelerate machine learning inference. For machine learning inference for applications like adding metadata to an image, object detection, recommender systems, automated speech recognition, and language translation. graphics-intensive workloads: For building and running graphics-intensive applications, such as remote graphics workstations, video transcoding, photo-realistic design, and game streaming in the cloud. I/O optimized (I3en): Non-Volatile Memory Express (NVMe) SSD instance storage optimized for low latency, high random I/O performance, high sequential disk throughput, and offers the lowest price per GB of SSD instance storage on Amazon EC2. Suited for NoSQL databases (Cassandra, MongoDB, Redis), in-memory databases (Aerospike), scale-out transactional databases, distributed file systems, data warehousing, Elasticsearch, analytics workloads. Storage Amazon EBS: local instance storage - that gets vanished when EC2 instance is stopped. Elastic Block Store (EBS) gp2 volumes for persistent block storage. snapshot and restore capabilities lets you increase volume size without any performance impact. All EBS volumes and snapshots on Outposts are fully encrypted by default. EBS is offered in tiers of 11 TB, 33 TB, and 55 TB. Amazon S3: Store, Retrieve Data on Outpost Secure Data Control Access Tags, Reports S3 APIs Add 26 TB, 48 TB, 96 TB, 240 TB, or 380 TB of S3 storage capacity Create up to 100 buckets per AWS account on each Outpost Amazon EBS Snapshot: a point-in-time copy of your EBS volumes. Snapshots of EBS volumes on your Outpost are stored on Amazon S3 in the Region in the region means in nearest cloud region? and not on local Outpost S3 storage? If you have S3 provisioned on your Outpost, then you can store EBS snapshot on Outpost S3 itself, it’s called EBS Local Snapshot. use EBS Local Snapshots on Outposts for disaster recovery and back up. Secure and protect data on EBS storage using resource-level IAM policies. Migration CloudEndure Migration: allows customers to migrate workloads onto AWS Outposts from physical, virtual, or cloud-based sources, from on-premises locations, public AWS Regions, and other clouds to Outposts. using EBS Local Snapshots on Outposts: migrate workloads from any source directly onto Outposts, or from one Outpost to another, without requiring the EBS snapshot data to go through the region. CloudEndure Disaster Recovery: business continuity solution for physical, virtual, and cloud-based workloads onto AWS Outposts. you can replicate and recover: from on-premises to Outposts from AWS Regions onto Outposts from Outposts into AWS Regions and between two Outposts. CloudEndure Disaster Recovery improves resilience, enabling recovery point objectives (RPOs) of seconds and recovery time objectives (RTOs) of minutes. Networking Extended VPC Extend your existing Amazon VPC to your Outpost in your on premises location. Create a subnet in your regional VPC and associate it with an Outpost just as you associate subnets with an Availability Zone in an AWS Region. Instances in Outpost subnets communicate with other instances in the AWS Region using private IP addresses, all within the same VPC. Local Gateway Each Outpost provides a new local gateway (LGW) that allows you to connect your Outpost resources with your on premises networks. LGW enables low latency connectivity between the Outpost and any local data sources, end users, local machinery and equipment, or local databases. Load Balancer You can provision an Application Load Balancer (ALB) to automatically distribute incoming HTTP(S) traffic across multiple targets on your Outposts, such as Amazon EC2 instances, containers, and IP addresses. ALB on Outposts is fully managed, operates in a single subnet, and scales automatically up to the capacity available on the Outposts rack to meet varying levels of application load without manual intervention. Private Connection to AWS Cloud With AWS Outposts Private Connectivity, you can establish a service link VPN connection &amp;lt;/u&amp;gt;from your Outposts to the AWS Region over&amp;lt;/u&amp;gt; AWS Direct Connect. Minimizes public internet exposure Removes the need for special firewall configurations. AWS Services on Outposts: Containers Amazon ECS: Scalable, High-performance container orchestration service, supports Docker containers run and scale containerized applications. ECS eliminates the need for: Install and Operate your own container orchestration software Manage and Scale a cluster of virtual machines Schedule containers on those virtual machines With simple API calls, you can launch and stop Docker-enabled applications and query the complete state of your application. Amazon EKS: Managed Service to run Kubernetes. Used to run Containerized applications. AWS ECS vs AWS EKS: Amazon ECS vs Amazon EKS: making sense of AWS container services Databases Amazon RDS (Relational Database Service) : supports Microsoft SQL Server, MySQL, and PostgreSQL database engines. Amazon RDS provides cost-efficient and resizable capacity while automating time-consuming administration tasks including infrastructure provisioning, database setup, patching, and backups. fully managed databases on premises Amazon RDS can be managed using AWS Management Console, APIs, and CLI as if in cloud. Amazon RDS enables low-cost, high-availability hybrid deployments, with disaster recovery back to the AWS Region, read replica bursting to Amazon RDS in the cloud, and long-term archival in Amazon Simple Storage Service (Amazon S3) in the cloud. Amazon ElastiCache: Fully managed in-memory data store, compatible with Redis or Memcached Optimized for real-time applications with sub-millisecond latency. Amazon ElastiCache on Outposts enables real-time use cases like Caching, Session Stores, Gaming, Geospatial Services, Real-Time Analytics, and Queuing. Data Analytics Amazon EMR: Deploys secure and managed EMR clusters. Deploys latest versions of Apache Spark, Apache Hive, and Presto to access critical on premises data sources and systems for big data analytics. Use the EMR console, SDK, or CLI to specify the subnet associated with your Outpost to launch EMR clusters. Upgrades to Outpost Services AWS services running locally on Outposts will be upgraded automatically to the latest version as and when available. Amazon RDS like services also patch both OS and database engines within scheduled maintenance windows with minimum downtime. Access to Cloud hosted Regional Services We can extend our Amazon Virtual Private Cloud(VPC) on premises and run some AWS services locally on Outposts and also connect to a broad range of services available in the local AWS Region. We can access all regional AWS services in your private VPC environment, for example, through Interface Endpoints, Gateway Endpoints, or their regional public endpoints. AWS Tools: With AWS Outposts, customers can access AWS tools running in the region such as, AWS CloudFormation, Amazon CloudWatch, AWS CloudTrail, Elastic BeanStalk, Cloud 9, and others to run and manage workloads on AWS Outposts the same way they do in the cloud. AWS Resource Access Manager AWS Resource Access Manager (RAM) lets customers share access to Outposts resources – EC2 instances, EBS volumes, S3 capacity, subnets, and local gateways (LGWs) – across multiple accounts under the same AWS organization. Further Readings AWS Outpost with AWS Greengrass Sample Architecture AWS Outpost Healthcare Sample Architecture Cloud at the Edge for Healthcare and Life Sciences Yet To Read: Deploying Containerized Application on AWS Outposts with Amazon EKS Building Modern Applications with Amazon EKS on Amazon Outposts Configuring an Application Load Balancer on AWS Outposts Managing your AWS Outposts capacity using Amazon CloudWatch and AWS Lambda Run applications on premises and access Amazon S3 objects from AWS Outposts Introducing AWS Outposts private connectivity</summary></entry><entry><title type="html">MQTT Protocol</title><link href="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/10/MQTT.html" rel="alternate" type="text/html" title="MQTT Protocol" /><published>2021-11-10T12:33:22+05:30</published><updated>2021-11-10T12:33:22+05:30</updated><id>http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/10/MQTT</id><content type="html" xml:base="http://localhost:4000/on-premise%20cloud/cloud/aws/edge%20iot/2021/11/10/MQTT.html">&lt;h1 id=&quot;mqtt-protocol&quot;&gt;MQTT Protocol&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;MQTT = Message Queuing Telemetry Transport
  Telemetry = Tele-Metering = Remote Measurements&lt;/li&gt;
  &lt;li&gt;Originally Developed by IBM, now Open Sourced.&lt;/li&gt;
  &lt;li&gt;Though MQ stands for ‘Message Queuing’, actually there’s NO Messages being Queued.&lt;/li&gt;
  &lt;li&gt;It’s a &lt;strong&gt;&lt;em&gt;&lt;u&gt;Publish/Subscribe&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; mechanism.
    &lt;ul&gt;
      &lt;li&gt;Sensor Devices Publish data to the Topics/Servers/Brokers.&lt;/li&gt;
      &lt;li&gt;Topics are subscribed by other devices such as mobile devices those are looking for data from Sensors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Low Bandwidth Protocol&lt;/u&gt;&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Messages being sent by devices are very small in bytes.&lt;/li&gt;
      &lt;li&gt;e.g. Temperature sensor sending 100 degree reading.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Small Code Footprint
    &lt;ul&gt;
      &lt;li&gt;The code used for this is very small.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Used in:
    &lt;ul&gt;
      &lt;li&gt;Facebook Messenger for iOS and Android&lt;/li&gt;
      &lt;li&gt;PubNub&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mqtt-ports&quot;&gt;MQTT Ports&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Protocol: TCP/IP&lt;/li&gt;
  &lt;li&gt;Ports:
    &lt;ul&gt;
      &lt;li&gt;1883 - non-encrypted communication&lt;/li&gt;
      &lt;li&gt;8883 - encrypted communication&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;maximum-payload-size&quot;&gt;Maximum Payload Size&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;MQTT Protocol Max Payload - 256 MBs&lt;/li&gt;
  &lt;li&gt;AWS IoT MQTT Max Payload - 128 KBs&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;quality-of-service&quot;&gt;Quality of Service&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Levels:&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;&lt;u&gt;0 = At Most Once&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (Best effort, No Ack)&lt;br /&gt;
      - Sender does not store messages, neither the receiver sends any acknowledgement.&lt;br /&gt;
      - This method requires only one message and once the message is sent to the broker by the client it is deleted from the message queue.&lt;br /&gt;
      - Therefore QoS 0 nullifies the chances of duplicate messages, which is why it is also known as the &lt;strong&gt;&lt;em&gt;&lt;u&gt;fire and forget&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; method.&lt;br /&gt;
      - It provides a minimal and most &lt;strong&gt;&lt;em&gt;&lt;u&gt;unreliable&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; message transmission level that offers the &lt;strong&gt;&lt;em&gt;&lt;u&gt;fastest delivery effort&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;.&lt;br /&gt;
      &lt;img src=&quot;/assets/images/mqtt/mqtt-qos-0.png&quot; alt=&quot;mqtt-qos-0&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;&lt;u&gt;1 = At Least Once&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (Acknowledged, Retransmitted if Ack not received)&lt;br /&gt;
      - Using QoS 1, the delivery of a message is guaranteed (at least once, but the &lt;em&gt;&lt;u&gt;message may be sent more than once&lt;/u&gt;&lt;/em&gt; , if necessary).&lt;br /&gt;
      - This method needs two messages.&lt;br /&gt;
      - Here, the sender sends a message and waits to receive &lt;em&gt;&lt;u&gt;an acknowledgment&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;&lt;u&gt;PUBACK&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; message).&lt;br /&gt;
      - If it receives an acknowledgment from the client then it deletes the message from the outward-bound queue.&lt;br /&gt;
      - In case, it does not receive a PUBACK message, it resends the message with the &lt;em&gt;&lt;u&gt;duplicate flag (DUP flag) enabled&lt;/u&gt;&lt;/em&gt;.&lt;br /&gt;
      &lt;img src=&quot;/assets/images/mqtt/mqtt-qos-1.png&quot; alt=&quot;mqtt-qos-1&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;
    &lt;strong&gt;&lt;em&gt;&lt;u&gt;2 = Exactly Once&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
      - The QoS 2 level setting guarantees exactly-once delivery of a message.&lt;br /&gt;
      - This is the &lt;em&gt;&lt;u&gt;slowest&lt;/u&gt;&lt;/em&gt; of all the levels and needs four messages.&lt;br /&gt;
      - In this level, the &lt;em&gt;&lt;u&gt;sender&lt;/u&gt;&lt;/em&gt; sends a message (&lt;strong&gt;&lt;em&gt;&lt;u&gt;PUBLISH&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;) and waits for an &lt;em&gt;&lt;u&gt;acknowledgment&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;&lt;u&gt;PUBREC&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; message).&lt;br /&gt;
      - The &lt;em&gt;&lt;u&gt;receiver&lt;/u&gt;&lt;/em&gt; also sends a PUBREC message.&lt;br /&gt;
      - If the &lt;em&gt;&lt;u&gt;sender&lt;/u&gt;&lt;/em&gt; of the message fails to receive an acknowledgment (PUBREC), it sends the message again with the &lt;em&gt;&lt;u&gt;DUP flag enabled&lt;/u&gt;&lt;/em&gt;.&lt;br /&gt;
      - Upon receiving the acknowledgment message PUBREC, the &lt;em&gt;&lt;u&gt;sender&lt;/u&gt;&lt;/em&gt; transmits the &lt;em&gt;&lt;u&gt;message release message&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;&lt;u&gt;PUBREL&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;).&lt;br /&gt;
      - If the &lt;em&gt;&lt;u&gt;receiver&lt;/u&gt;&lt;/em&gt; does not receive the PUBREL message it resends the PUBREC message.&lt;br /&gt;
      - Once the &lt;em&gt;&lt;u&gt;receiver&lt;/u&gt;&lt;/em&gt; receives the PUBREL message, It forwards the message to all the subscribing clients.&lt;br /&gt;
      - Note: &lt;em&gt;&lt;u&gt;For Sender Broker is the receiver and for Receiver Broker is the sender&lt;/u&gt;&lt;/em&gt;.&lt;br /&gt;
      - Thereafter the &lt;em&gt;&lt;u&gt;receiver&lt;/u&gt;&lt;/em&gt; sends &lt;em&gt;&lt;u&gt;a publish complete&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;&lt;em&gt;&lt;u&gt;PUBCOMP&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;) message.&lt;br /&gt;
      - In case the &lt;em&gt;&lt;u&gt;sender&lt;/u&gt;&lt;/em&gt; does not receive the PUBCOMP message, it resends the PUBREL message.&lt;br /&gt;
      - Once the sending client receives the PUBCOMP message, the transmission process is marked as completed and the message
can be deleted from the outbound queue.&lt;br /&gt;
  &lt;img src=&quot;/assets/images/mqtt/mqtt-qos-2.png&quot; alt=&quot;mqtt-qos-2&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;these messages are same as used in WiFi communication.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;retained-messages&quot;&gt;Retained Messages&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Server keeps messages even after sending it to all Subscribers.&lt;/li&gt;
  &lt;li&gt;New Subscribers get the retained messages.
    &lt;ul&gt;
      &lt;li&gt;We had seen this behavior in PubNub, where last message Published to Channel was retained till the next message is Published to that channel.&lt;/li&gt;
      &lt;li&gt;This Last message was available to new subscriber.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;&lt;u&gt;Only One Message&lt;/u&gt;&lt;/em&gt; is retained per Topic.&lt;/li&gt;
      &lt;li&gt;Usecase:
        &lt;ul&gt;
          &lt;li&gt;Sensor periodically sending a message on a topic. New subscriber will get the last state of the sensor with Retained message.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Reference: &lt;a href=&quot;/assets/docs/mqtt/MQTT_Retained_Messages_Explained.pdf&quot;&gt;MQTT Retained Messages Explained&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;clean-session-and-durable-session&quot;&gt;Clean Session and Durable Session&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Clean Session&lt;/u&gt;&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;Clean Session Flag = 1	[Optional]&lt;/li&gt;
      &lt;li&gt;All of the client’s subscriptions are removed when it disconnects from the server.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Durable Session&lt;/u&gt;&lt;/em&gt;:
    &lt;ul&gt;
      &lt;li&gt;Clean Session Flag = 0	[Optional]&lt;/li&gt;
      &lt;li&gt;The client’s subscriptions remain in effect after any disconnection.&lt;/li&gt;
      &lt;li&gt;In this event, subsequent messages that arrive carrying a High QoS designation are stored for delivery after the connection is reestablished.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;retained-messages-clean-session-and-qos-inter-related-behavior&quot;&gt;Retained messages, Clean Session and QoS inter-related behavior&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/mqtt/RetainedMessages_CleanSession_QoS.png&quot; alt=&quot;Retained messages, Clean Session and QoS inter-related behavior&quot; /&gt;&lt;br /&gt;
  Reference: &lt;a href=&quot;http://www.steves-internet-guide.com/mqtt-retained-messages-example/&quot;&gt;MQTT Retained Messages Explained&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;wills&quot;&gt;Wills&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;A Will or a Message is informed by client with server that should be published to a specific Topic or Topics in the event of an unexpected disconnection.&lt;/li&gt;
  &lt;li&gt;A Will is an alarm or security settings where system when a remote sensor has lost contact with the network.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;keep-alive-messages&quot;&gt;Keep Alive Messages&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Periodically sent&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;topic-trees-strings&quot;&gt;Topic Trees, Strings&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Topics are organized Hierarchically into Topic Trees, using the ‘/’ character to create subtopics in the Topic String.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;u&gt;Topic String&lt;/u&gt;&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;A character string that identifies the Topic of a publish/subscribe message.&lt;/li&gt;
      &lt;li&gt;Topic strings can contain either of two Wildcards:&lt;/li&gt;
      &lt;li&gt;These &lt;strong&gt;&lt;em&gt;&lt;u&gt;WildCards&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; Allows subscribers to match patterns within strings defined by message publishers&lt;/li&gt;
      &lt;li&gt;Wildcard: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt;:
        &lt;ul&gt;
          &lt;li&gt;Multilevel&lt;/li&gt;
          &lt;li&gt;used to match any number of levels within a Topic.
            &lt;ul&gt;
              &lt;li&gt;e.g. subscribers to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck/contents/#&lt;/code&gt; receive all messages that are designated for the topics:&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck/contents&lt;/code&gt;&lt;/li&gt;
              &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck/contents/rfid&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Wildcard: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt;:
        &lt;ul&gt;
          &lt;li&gt;Single Level&lt;/li&gt;
          &lt;li&gt;used to match Just ONE Topic Level.
            &lt;ul&gt;
              &lt;li&gt;e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck/+&lt;/code&gt; &lt;u&gt;matches&lt;/u&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck/contents&lt;/code&gt; &lt;em&gt;&lt;u&gt;but not&lt;/u&gt;&lt;/em&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;truck/contents/rfid&lt;/code&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mqtt-brokers-comparison&quot;&gt;MQTT Brokers Comparison&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/assets/docs/mqtt/Stress-Testing-MQTT-Brokers.pdf&quot;&gt;Stress-Testing MQTT Brokers: A Comparative Analysis of Performance Measurements:&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Tested Borkers:
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://mosquitto.org/&quot;&gt;&lt;em&gt;&lt;u&gt;Mosquitto&lt;/u&gt;&lt;/em&gt;&lt;/a&gt; - &lt;a href=&quot;https://www.bevywise.com/mqtt-broker/&quot;&gt;&lt;em&gt;&lt;u&gt;Bevywise MQTT Route&lt;/u&gt;&lt;/em&gt;&lt;/a&gt; - &lt;a href=&quot;https://activemq.apache.org/&quot;&gt;&lt;em&gt;&lt;u&gt;ActiveMQ&lt;/u&gt;&lt;/em&gt;&lt;/a&gt; - &lt;a href=&quot;https://www.hivemq.com/&quot;&gt;&lt;em&gt;&lt;u&gt;HiveMQ CE&lt;/u&gt;&lt;/em&gt;&lt;/a&gt; - &lt;a href=&quot;https://vernemq.com/&quot;&gt;&lt;em&gt;&lt;u&gt;VerneMQ&lt;/u&gt;&lt;/em&gt;&lt;/a&gt; - &lt;a href=&quot;https://www.emqx.io/&quot;&gt;&lt;em&gt;&lt;u&gt;EMQ X&lt;/u&gt;&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Mosquitto&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; outperforms the other considered solutions in most metrics;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;ActiveMQ&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; is the best performing one in terms of &lt;em&gt;&lt;u&gt;Scalability&lt;/u&gt;&lt;/em&gt; due to its multi-threaded implementation.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;Bevywise MQTT Route&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; has promising results for &lt;em&gt;&lt;u&gt;resource-constrained&lt;/u&gt;&lt;/em&gt; scenarios.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;&lt;u&gt;ActiveMQ&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; scales well in distributed/multi-core environment to beat all other brokers’ performance.&lt;/li&gt;
      &lt;li&gt;If the hardware is &lt;em&gt;&lt;u&gt;resource-constrained&lt;/u&gt;&lt;/em&gt; (CPU/Memory/IO/Performance), then &lt;strong&gt;&lt;em&gt;&lt;u&gt;Mosquitto&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; or &lt;strong&gt;&lt;em&gt;&lt;u&gt;Bevywise MQTT Route&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; can be taken as better choices.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;running-mqtt-broker-in-containerized-mode&quot;&gt;Running MQTT broker in Containerized mode&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hub.docker.com/_/eclipse-mosquitto?tab=description&quot;&gt;eclipse-mosquitto&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;Maintained by: the Eclipse Foundation&lt;/li&gt;
      &lt;li&gt;Supported architectures: (more info) amd64, arm32v6, arm64v8, i386, ppc64le, s390x&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><category term="On-Premise Cloud" /><category term="Cloud" /><category term="AWS" /><category term="Edge IoT" /><summary type="html">MQTT Protocol MQTT = Message Queuing Telemetry Transport Telemetry = Tele-Metering = Remote Measurements Originally Developed by IBM, now Open Sourced. Though MQ stands for ‘Message Queuing’, actually there’s NO Messages being Queued. It’s a Publish/Subscribe mechanism. Sensor Devices Publish data to the Topics/Servers/Brokers. Topics are subscribed by other devices such as mobile devices those are looking for data from Sensors. Low Bandwidth Protocol Messages being sent by devices are very small in bytes. e.g. Temperature sensor sending 100 degree reading. Small Code Footprint The code used for this is very small. Used in: Facebook Messenger for iOS and Android PubNub MQTT Ports Protocol: TCP/IP Ports: 1883 - non-encrypted communication 8883 - encrypted communication Maximum Payload Size MQTT Protocol Max Payload - 256 MBs AWS IoT MQTT Max Payload - 128 KBs Quality of Service Levels: 0 = At Most Once (Best effort, No Ack) - Sender does not store messages, neither the receiver sends any acknowledgement. - This method requires only one message and once the message is sent to the broker by the client it is deleted from the message queue. - Therefore QoS 0 nullifies the chances of duplicate messages, which is why it is also known as the fire and forget method. - It provides a minimal and most unreliable message transmission level that offers the fastest delivery effort. 1 = At Least Once (Acknowledged, Retransmitted if Ack not received) - Using QoS 1, the delivery of a message is guaranteed (at least once, but the message may be sent more than once , if necessary). - This method needs two messages. - Here, the sender sends a message and waits to receive an acknowledgment (PUBACK message). - If it receives an acknowledgment from the client then it deletes the message from the outward-bound queue. - In case, it does not receive a PUBACK message, it resends the message with the duplicate flag (DUP flag) enabled. 2 = Exactly Once - The QoS 2 level setting guarantees exactly-once delivery of a message. - This is the slowest of all the levels and needs four messages. - In this level, the sender sends a message (PUBLISH) and waits for an acknowledgment (PUBREC message). - The receiver also sends a PUBREC message. - If the sender of the message fails to receive an acknowledgment (PUBREC), it sends the message again with the DUP flag enabled. - Upon receiving the acknowledgment message PUBREC, the sender transmits the message release message (PUBREL). - If the receiver does not receive the PUBREL message it resends the PUBREC message. - Once the receiver receives the PUBREL message, It forwards the message to all the subscribing clients. - Note: For Sender Broker is the receiver and for Receiver Broker is the sender. - Thereafter the receiver sends a publish complete (PUBCOMP) message. - In case the sender does not receive the PUBCOMP message, it resends the PUBREL message. - Once the sending client receives the PUBCOMP message, the transmission process is marked as completed and the message can be deleted from the outbound queue. these messages are same as used in WiFi communication. Retained Messages Server keeps messages even after sending it to all Subscribers. New Subscribers get the retained messages. We had seen this behavior in PubNub, where last message Published to Channel was retained till the next message is Published to that channel. This Last message was available to new subscriber. Only One Message is retained per Topic. Usecase: Sensor periodically sending a message on a topic. New subscriber will get the last state of the sensor with Retained message. Reference: MQTT Retained Messages Explained Clean Session and Durable Session Clean Session: Clean Session Flag = 1 [Optional] All of the client’s subscriptions are removed when it disconnects from the server. Durable Session: Clean Session Flag = 0 [Optional] The client’s subscriptions remain in effect after any disconnection. In this event, subsequent messages that arrive carrying a High QoS designation are stored for delivery after the connection is reestablished. Retained messages, Clean Session and QoS inter-related behavior Reference: MQTT Retained Messages Explained Wills A Will or a Message is informed by client with server that should be published to a specific Topic or Topics in the event of an unexpected disconnection. A Will is an alarm or security settings where system when a remote sensor has lost contact with the network. Keep Alive Messages Periodically sent Topic Trees, Strings Topics are organized Hierarchically into Topic Trees, using the ‘/’ character to create subtopics in the Topic String. Topic String A character string that identifies the Topic of a publish/subscribe message. Topic strings can contain either of two Wildcards: These WildCards Allows subscribers to match patterns within strings defined by message publishers Wildcard: #: Multilevel used to match any number of levels within a Topic. e.g. subscribers to truck/contents/# receive all messages that are designated for the topics: truck/contents truck/contents/rfid Wildcard: +: Single Level used to match Just ONE Topic Level. e.g. truck/+ matches truck/contents but not truck/contents/rfid MQTT Brokers Comparison Stress-Testing MQTT Brokers: A Comparative Analysis of Performance Measurements: Tested Borkers: Mosquitto - Bevywise MQTT Route - ActiveMQ - HiveMQ CE - VerneMQ - EMQ X Mosquitto outperforms the other considered solutions in most metrics; ActiveMQ is the best performing one in terms of Scalability due to its multi-threaded implementation. Bevywise MQTT Route has promising results for resource-constrained scenarios. ActiveMQ scales well in distributed/multi-core environment to beat all other brokers’ performance. If the hardware is resource-constrained (CPU/Memory/IO/Performance), then Mosquitto or Bevywise MQTT Route can be taken as better choices. Running MQTT broker in Containerized mode eclipse-mosquitto Maintained by: the Eclipse Foundation Supported architectures: (more info) amd64, arm32v6, arm64v8, i386, ppc64le, s390x</summary></entry><entry><title type="html">C++ STL: Container Adapters and Use cases</title><link href="http://localhost:4000/c++/data%20structures/stl/2021/11/01/C++_STL_Container_Adapters.html" rel="alternate" type="text/html" title="C++ STL: Container Adapters and Use cases" /><published>2021-11-01T12:33:22+05:30</published><updated>2021-11-01T12:33:22+05:30</updated><id>http://localhost:4000/c++/data%20structures/stl/2021/11/01/C++_STL_Container_Adapters</id><content type="html" xml:base="http://localhost:4000/c++/data%20structures/stl/2021/11/01/C++_STL_Container_Adapters.html">&lt;p&gt;This write-up will explain different &lt;strong&gt;&lt;em&gt;&lt;u&gt;Container Adapters&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, like &lt;strong&gt;&lt;em&gt;&lt;u&gt;Stack&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;&lt;u&gt;Queue&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;&lt;u&gt;Priority Queue&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; their respective features and mainly their appropriate use-cases.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Container Adapters are NOT full container classes.&lt;/li&gt;
  &lt;li&gt;But the classes that provide a &lt;em&gt;&lt;u&gt;specific interface&lt;/u&gt;&lt;/em&gt; relying on an object of one of the Container class (such as deque/list) to handle the elements.&lt;/li&gt;
  &lt;li&gt;The underlying container is encapsulated in a way that its elements are accessed by the members of the container adapter independent of the underlying container class.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;stack&quot;&gt;Stack&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Operate in a &lt;strong&gt;&lt;em&gt;&lt;u&gt;LIFO&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; context (&lt;em&gt;&lt;u&gt;last-in first-out&lt;/u&gt;&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Elements are &lt;strong&gt;Inserted/Extracted&lt;/strong&gt; &lt;em&gt;&lt;u&gt;Only from&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;ONE END&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; of the container.&lt;/li&gt;
  &lt;li&gt;Elements are &lt;strong&gt;pushed/popped&lt;/strong&gt; from the &lt;em&gt;&lt;u&gt;back&lt;/u&gt;&lt;/em&gt; of the specific container, which is known as the &lt;em&gt;&lt;u&gt;TOP&lt;/u&gt;&lt;/em&gt; of the stack.&lt;/li&gt;
  &lt;li&gt;By &lt;em&gt;&lt;u&gt;default&lt;/u&gt;&lt;/em&gt;, standard container &lt;strong&gt;std::deque&lt;/strong&gt; is used, if no container class is specified for a particular stack class instantiation.&lt;/li&gt;
  &lt;li&gt;Otherwise, &lt;strong&gt;std::vector&lt;/strong&gt;, &lt;strong&gt;std::list&lt;/strong&gt; also used as underlying container.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;Use-cases&lt;/u&gt;:
    &lt;ul&gt;
      &lt;li&gt;First-In Last-Out operations&lt;/li&gt;
      &lt;li&gt;Reversal of elements&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;std::stack&amp;lt;int&amp;gt; first;                                  // empty stack
---------------------
    
std::deque&amp;lt;int&amp;gt; mydeque (3,100);                        // deque with 3 elements
std::stack&amp;lt;int&amp;gt; second (mydeque);                       // stack initialized to copy of deque
---------------------
    
std::stack&amp;lt;int,std::vector&amp;lt;int&amp;gt; &amp;gt; third;                // empty stack using vector
---------------------
    
std::vector&amp;lt;int&amp;gt; myvector (2,200);                      // vector with 2 elements
std::stack&amp;lt;int,std::vector&amp;lt;int&amp;gt; &amp;gt; fourth (myvector);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;operations&quot;&gt;Operations:&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    * empty()
    * size()
    * top()
    * pop()
    * push()
    ------------
    
    C++11:
    * emplace()
    * swap()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;time-complexity-stack-operations&quot;&gt;Time Complexity Stack Operations&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Operation&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time Complexity&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Push&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;&lt;em&gt;O(1)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;&lt;em&gt;O(1)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Top&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;&lt;em&gt;O(1)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;stack-decision-criteria&quot;&gt;Stack Decision Criteria&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;:heavy_check_mark: &lt;strong&gt;Order&lt;/strong&gt; is Important.&lt;/li&gt;
  &lt;li&gt;:heavy_check_mark: &lt;em&gt;&lt;u&gt;Last-in, First-out&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;LIFO&lt;/strong&gt;) accepted/required.&lt;/li&gt;
  &lt;li&gt;:x: Size varies widely - &lt;strong&gt;Dynamic&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;:x: Need to find Nth element - &lt;strong&gt;Random Access&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Insert/Erase&lt;/strong&gt; at the &lt;em&gt;&lt;u&gt;Front/Back&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Insert/Erase&lt;/strong&gt; at &lt;em&gt;&lt;u&gt;Mid-positions&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Key:Value&lt;/strong&gt; pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;references&quot;&gt;References:&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cplusplus.com/reference/stack/stack/&quot;&gt;std::stack&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;queue&quot;&gt;Queue&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Operate in &lt;strong&gt;&lt;em&gt;&lt;u&gt;FIFO&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; (&lt;em&gt;&lt;u&gt;First-in, First-out&lt;/u&gt;&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;Elements are &lt;strong&gt;Inserted&lt;/strong&gt; &lt;em&gt;&lt;u&gt;from one end&lt;/u&gt;&lt;/em&gt; of the container and &lt;strong&gt;Extracted&lt;/strong&gt; &lt;em&gt;&lt;u&gt;from the other end&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Elements are &lt;strong&gt;Pushed&lt;/strong&gt; from the &lt;em&gt;&lt;u&gt;back&lt;/u&gt;&lt;/em&gt; of the specific container and &lt;strong&gt;Popped&lt;/strong&gt; from its &lt;em&gt;&lt;u&gt;front&lt;/u&gt;&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;By &lt;em&gt;&lt;u&gt;default&lt;/u&gt;&lt;/em&gt;, standard container ‘&lt;strong&gt;&lt;em&gt;std::deque&lt;/em&gt;&lt;/strong&gt;’ is used, if no container class is specified for a particular queue class instantiation.&lt;/li&gt;
  &lt;li&gt;Otherwise, &lt;strong&gt;&lt;em&gt;std::vector&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;std::list&lt;/em&gt;&lt;/strong&gt; also used as underlying container.&lt;/li&gt;
  &lt;li&gt;&lt;u&gt;Use-cases&lt;/u&gt;:
    &lt;ul&gt;
      &lt;li&gt;First-In First-Out operations&lt;/li&gt;
      &lt;li&gt;Simple online ordering system (first come first served)&lt;/li&gt;
      &lt;li&gt;Semaphore queue handling&lt;/li&gt;
      &lt;li&gt;CPU scheduling (FCFS)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Example:
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;std::queue&amp;lt;int&amp;gt; first;                                // empty queue
------------
    
std::deque&amp;lt;int&amp;gt; mydeck (3,100);                       // deque with 3 elements
std::queue&amp;lt;int&amp;gt; second (mydeck);                      // queue initialized to copy of deque
------------
    
std::queue&amp;lt;int,std::list&amp;lt;int&amp;gt; &amp;gt; third;                // empty queue with list as underlying container
------------
    
std::list&amp;lt;int&amp;gt; mylist (2,200);                        // list with 2 elements
std::queue&amp;lt;int,std::list&amp;lt;int&amp;gt; &amp;gt; fourth (mylist);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;operations-1&quot;&gt;Operations:&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    * empty()
    * size()
    * front()
    * back()
    * pop()
    * push()
    --------------
    
    C++11:
    * emplace()
    * swap()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;time-complexity-queue-operations&quot;&gt;Time Complexity Queue Operations&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Operation&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Time Complexity&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Push&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;&lt;em&gt;O(1)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Pop&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;&lt;em&gt;O(1)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Top&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;&lt;em&gt;O(1)&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;queue-decision-criteria&quot;&gt;Queue Decision Criteria&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;:heavy_check_mark: &lt;strong&gt;Order&lt;/strong&gt; is Important.&lt;/li&gt;
  &lt;li&gt;:heavy_check_mark: &lt;em&gt;&lt;u&gt;First-in, First-out&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;FIFO&lt;/strong&gt;) accepted/required.&lt;/li&gt;
  &lt;li&gt;:x: Size varies widely - &lt;strong&gt;Dynamic&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;:x: Need to find Nth element - &lt;strong&gt;Random Access&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Insert/Erase&lt;/strong&gt; at the &lt;em&gt;&lt;u&gt;Front/Back&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Insert/Erase&lt;/strong&gt; at &lt;em&gt;&lt;u&gt;Mid-positions&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Key:Value&lt;/strong&gt; pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;priority_queue&quot;&gt;Priority_Queue&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Its First element is always the “&lt;strong&gt;&lt;em&gt;&lt;u&gt;Greatest of the elements&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt;” it contains, according to some “&lt;em&gt;&lt;u&gt;strict weak ordering&lt;/u&gt;&lt;/em&gt;” criterion.
    &lt;ul&gt;
      &lt;li&gt;Similar to a &lt;em&gt;&lt;u&gt;heap&lt;/u&gt;&lt;/em&gt;, where elements can be inserted at any moment, and &lt;em&gt;&lt;u&gt;only the&lt;/u&gt;&lt;/em&gt; &lt;strong&gt;&lt;em&gt;&lt;u&gt;max heap&lt;/u&gt;&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;&lt;u&gt;element&lt;/u&gt;&lt;/em&gt; can be retrieved (the one at the top in the priority queue).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Elements are &lt;strong&gt;Popped&lt;/strong&gt; from the &lt;em&gt;&lt;u&gt;back&lt;/u&gt;&lt;/em&gt; of the specific container, which is known as the &lt;em&gt;&lt;u&gt;top&lt;/u&gt;&lt;/em&gt; of the priority queue.&lt;/li&gt;
  &lt;li&gt;The underlying container shall be accessible through &lt;strong&gt;Random Access&lt;/strong&gt; Iterators
    &lt;ul&gt;
      &lt;li&gt;Support of Random Access Iterators is required to keep a “&lt;em&gt;&lt;u&gt;Heap structure&lt;/u&gt;&lt;/em&gt;” internally at all times.&lt;/li&gt;
      &lt;li&gt;This is done automatically by the container adapter by automatically calling the algorithm functions:
        &lt;ul&gt;
          &lt;li&gt;make_heap,&lt;/li&gt;
          &lt;li&gt;push_heap and&lt;/li&gt;
          &lt;li&gt;pop_heap&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;By &lt;em&gt;&lt;u&gt;default&lt;/u&gt;&lt;/em&gt;, standard container ‘&lt;strong&gt;&lt;em&gt;std::vector&lt;/em&gt;&lt;/strong&gt;’ is used, if no container class is specified for a particular priority_queue class instantiation.&lt;/li&gt;
  &lt;li&gt;Otherwise, ‘&lt;strong&gt;std::deque&lt;/strong&gt;’ can also be used as underlying container.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;operation&quot;&gt;Operation:&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    * empty()
    * size()
    * top()
    * push()
    * pop()
    ------------
    
    C++11:
    * emplace()
    * swap()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;priority-queue-decision-criteria&quot;&gt;Priority Queue Decision Criteria&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;:heavy_check_mark: &lt;strong&gt;Order&lt;/strong&gt; is Important.&lt;/li&gt;
  &lt;li&gt;:heavy_check_mark: &lt;em&gt;&lt;u&gt;First-in, First-out&lt;/u&gt;&lt;/em&gt; (&lt;strong&gt;FIFO&lt;/strong&gt;) accepted/required.&lt;/li&gt;
  &lt;li&gt;:x: Size varies widely - &lt;strong&gt;Dynamic&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;:x: Need to find Nth element - &lt;strong&gt;Random Access&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Insert/Erase&lt;/strong&gt; at the &lt;em&gt;&lt;u&gt;Front/Back&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Insert/Erase&lt;/strong&gt; at &lt;em&gt;&lt;u&gt;Mid-positions&lt;/u&gt;&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;:x: &lt;strong&gt;Key:Value&lt;/strong&gt; pair&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;reference&quot;&gt;Reference&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cplusplus.com/reference/stack/stack/&quot;&gt;std::stack&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cplusplus.com/reference/queue/queue/&quot;&gt;std::queue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cplusplus.com/reference/queue/priority_queue/&quot;&gt;std::priority_queue&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Akhilesh Moghe</name></author><category term="C++" /><category term="Data Structures" /><category term="STL" /><category term="C++" /><category term="Data Structures" /><category term="STL" /><summary type="html">This write-up will explain different Container Adapters, like Stack, Queue, Priority Queue their respective features and mainly their appropriate use-cases. Container Adapters are NOT full container classes. But the classes that provide a specific interface relying on an object of one of the Container class (such as deque/list) to handle the elements. The underlying container is encapsulated in a way that its elements are accessed by the members of the container adapter independent of the underlying container class. Stack Operate in a LIFO context (last-in first-out) Elements are Inserted/Extracted Only from ONE END of the container. Elements are pushed/popped from the back of the specific container, which is known as the TOP of the stack. By default, standard container std::deque is used, if no container class is specified for a particular stack class instantiation. Otherwise, std::vector, std::list also used as underlying container. Use-cases: First-In Last-Out operations Reversal of elements Example: std::stack&amp;lt;int&amp;gt; first; // empty stack --------------------- std::deque&amp;lt;int&amp;gt; mydeque (3,100); // deque with 3 elements std::stack&amp;lt;int&amp;gt; second (mydeque); // stack initialized to copy of deque --------------------- std::stack&amp;lt;int,std::vector&amp;lt;int&amp;gt; &amp;gt; third; // empty stack using vector --------------------- std::vector&amp;lt;int&amp;gt; myvector (2,200); // vector with 2 elements std::stack&amp;lt;int,std::vector&amp;lt;int&amp;gt; &amp;gt; fourth (myvector); Operations: * empty() * size() * top() * pop() * push() ------------ C++11: * emplace() * swap() Time Complexity Stack Operations Operation Time Complexity Push O(1) Pop O(1) Top O(1) Stack Decision Criteria :heavy_check_mark: Order is Important. :heavy_check_mark: Last-in, First-out (LIFO) accepted/required. :x: Size varies widely - Dynamic :x: Need to find Nth element - Random Access :x: Insert/Erase at the Front/Back :x: Insert/Erase at Mid-positions :x: Key:Value pair References: std::stack Queue Operate in FIFO (First-in, First-out) Elements are Inserted from one end of the container and Extracted from the other end. Elements are Pushed from the back of the specific container and Popped from its front. By default, standard container ‘std::deque’ is used, if no container class is specified for a particular queue class instantiation. Otherwise, std::vector, std::list also used as underlying container. Use-cases: First-In First-Out operations Simple online ordering system (first come first served) Semaphore queue handling CPU scheduling (FCFS) Example: std::queue&amp;lt;int&amp;gt; first; // empty queue ------------ std::deque&amp;lt;int&amp;gt; mydeck (3,100); // deque with 3 elements std::queue&amp;lt;int&amp;gt; second (mydeck); // queue initialized to copy of deque ------------ std::queue&amp;lt;int,std::list&amp;lt;int&amp;gt; &amp;gt; third; // empty queue with list as underlying container ------------ std::list&amp;lt;int&amp;gt; mylist (2,200); // list with 2 elements std::queue&amp;lt;int,std::list&amp;lt;int&amp;gt; &amp;gt; fourth (mylist); Operations: * empty() * size() * front() * back() * pop() * push() -------------- C++11: * emplace() * swap() Time Complexity Queue Operations Operation Time Complexity Push O(1) Pop O(1) Top O(1) Queue Decision Criteria :heavy_check_mark: Order is Important. :heavy_check_mark: First-in, First-out (FIFO) accepted/required. :x: Size varies widely - Dynamic :x: Need to find Nth element - Random Access :x: Insert/Erase at the Front/Back :x: Insert/Erase at Mid-positions :x: Key:Value pair Priority_Queue Its First element is always the “Greatest of the elements” it contains, according to some “strict weak ordering” criterion. Similar to a heap, where elements can be inserted at any moment, and only the max heap element can be retrieved (the one at the top in the priority queue). Elements are Popped from the back of the specific container, which is known as the top of the priority queue. The underlying container shall be accessible through Random Access Iterators Support of Random Access Iterators is required to keep a “Heap structure” internally at all times. This is done automatically by the container adapter by automatically calling the algorithm functions: make_heap, push_heap and pop_heap By default, standard container ‘std::vector’ is used, if no container class is specified for a particular priority_queue class instantiation. Otherwise, ‘std::deque’ can also be used as underlying container. Operation: * empty() * size() * top() * push() * pop() ------------ C++11: * emplace() * swap() Priority Queue Decision Criteria :heavy_check_mark: Order is Important. :heavy_check_mark: First-in, First-out (FIFO) accepted/required. :x: Size varies widely - Dynamic :x: Need to find Nth element - Random Access :x: Insert/Erase at the Front/Back :x: Insert/Erase at Mid-positions :x: Key:Value pair Reference std::stack std::queue std::priority_queue</summary></entry></feed>